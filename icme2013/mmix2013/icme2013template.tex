% Template for ICME-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,epsfig}

\usepackage[utf8]{inputenc}

\usepackage{subfig}
\usepackage{enumerate}

% autoref command
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\def\subfigureautorefname{Subfigure}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=ยง,
        aboveskip=2em,belowskip=1em,abovecaptionskip=0.5em,belowcaptionskip=0.5em,
        framexbottommargin=-1em,basicstyle=\ttfamily\listingsize\selectfont}

\pagestyle{empty}

\begin{document}\sloppy

% for 1st, 2nd, etc. superscripting
\newcommand{\ts}{\textsuperscript}

% Title.
% ------
\title{Tell me why! Ain't nothin' but a~mistake?\\ Describing Media Item Differences with Media Fragments URI}
%
% Single address.
% ---------------
\name{Anonymous ICME submission}
\address{}

\maketitle

%
\begin{abstract}
We have developed a~tile-wise histogram-based
media item deduplication and clustering algorithm
with additional high-level semantic matching criteria
that is tailored to photos and videos stemming from multiple social networks.
In this paper, we investigate whether the addressing scheme
Media Fragments  {\sc uri} provides a~feasible and practicable way
to vividly and graphically describe media item differences
between media items of type photo and/or video.
\end{abstract}
%
\begin{keywords}
Media Fragments {\sc uri}, Media Fragments, Media Items, Deduplication, Social Networks
\end{keywords}
%
\section{Introduction}
\label{sec:introduction}

The \emph{Backstreet Boys}~({\sc bsb}) are a~boy band
formed in~1993 in Orlando,~FL
that has sold over 130~million records worldwide,
making them the best-selling boy band of all time.
In~2013, the band will celebrate their 20\ts{th}~anniversary
with a~new album and a~world tour.
Reason enough for us to make them titular saint of this paper
with their hit song \emph{I~Want It That Way}
from the album \emph{Millennium}.
While the spike of their career was in the late 90s,
even today, people still actively share,%
\footnote{{\sc bsb} on social networks: \url{http://bit.ly/backstreet-gplus}
and \url{http://bit.ly/backstreet-fb},
both accessed 03/04/2013}
publish, and follow the group on \emph{social networks}.

\subsection{Previous Work}
\label{sec:previous-work}

Social networks are at the heart of our research on event summarization,
specifically deduplicating \emph{exact-} and \emph{near-duplicate}
media items that optionally accompany textual status messages
referred to as \emph{microposts} on multiple social networks. 
In the context of our research, we define a~\emph{media item}
as either a~photo (image) or video
that was \emph{publicly} shared or published
on at least one social network.
\autoref{fig:near-duplicate} shows an example
where two users of the social networks Facebook and Google+
independently of each other share a~\emph{near-duplicate} media item
in form of the music video \emph{Everybody}
performed by the \emph{Backstreet Boys}.
In order to detect, deduplicate, and cluster such occurrences
of \emph{exact-} and \emph{near-duplicate}
media items being shared independently across social networks,
we have implemented a~tile-wise histogram-based algorithm
with additional high-level semantic matching criteria
that was shown to work effectively and efficiently for several events.

\begin{figure}[b!]
  \centering
  \includegraphics[width=0.75\linewidth]{./backstreetboys.png}
  \caption{\emph{Near-duplicate} music video \emph{Everybody}
    by the \emph{Backstreet Boys} shared
    independently on Facebook and Google+}
  \label{fig:near-duplicate}
\end{figure}

\subsection{Motivation and Research Question}
\label{sec:motivation-and-research-question}

During previous experiments on clustering event-related media items,
we noticed that human raters wanted to know \emph{why}%
\footnote{Tell me why! Ain't nothin' but a~mistake?}
certain media items were clustered as \emph{exact-} or \emph{near-duplicates}.
In consequence, in this paper, we investigate in how far
Media Fragments {\sc uri}~\cite{troncy2012mediafragments}
provides a~feasible and practicable way
to tell raters why media items were clustered.
As we deal with media items of type photo and/or video,
we make simultaneous use of two types of media fragment dimensions,
the temporal dimension and the spatial dimension.

\subsection{Paper Structure}
\label{sec:paper-structure}

The remainder of this paper is structured as follows.
In \autoref{sec:related-work}, we report on related work
on media fragments and digital storytelling.
In \autoref{sec:media-fragment-requirements}, we describe
our requirements on media fragment identifiers.
In \autoref{sec:media-item-deduplication-algorithm},
we detail how the media item deduplication algorithm works
and show low-level debugging approaches to check
\emph{if} or \emph{if not} media items are clustered.
In \autoref{sec:from-debug-output-to-story},
we elaborate on how this low-level debug output gets lifted
to natural language stories for human raters to understand
\emph{why} or \emph{why not} media items are clustered.
The chosen approach gets evaluated in \autoref{sec:evaluation}.
We conclude in \autoref{sec:conclusions-and-future-work}
and give an outlook on future work.

\section{Related Work}
\label{sec:related-work}

\noindent \textit{Media Fragments:}
There are many online video hosting platforms
that have some sort of media fragment support.
In the following, we present two representative ones.
The video hosting platform YouTube%
\footnote{YouTube: \url{http://www.youtube.com/}}
allows for deep-linking into videos
via a~proprietary {\sc url} parameter \texttt{t},
whose value has to match the regular expression
\texttt{\textbackslash d+m\textbackslash d+s} (for minutes and seconds),
as documented in~\cite{youtube2008link}.
Dailymotion\footnote{Dailymotion: \url{http://www.dailymotion.com/}}
has a~similar, albeit undocumented, {\sc url} parameter \texttt{start},
whose value has to match the regular expression
\texttt{\textbackslash d+} (for seconds).
The {\sc css} Backgrounds and Borders Module Level~3 specification~\cite{bos2012css3}
defines the \texttt{background-size} property
that, combined with a wrapping element,
can be used to crop media items visually
and thus create the illusion of a~spatial media fragment.
Media Fragments {\sc uri}~\cite{troncy2012mediafragments} specifies
a~syntax for constructing media fragment {\sc uri}s
and explains how to handle them
when used over the {\sc http} protocol~\cite{fielding1999http}.
The syntax is based on the specification of particular name-value pairs
that can be used in {\sc uri} fragment and {\sc uri} query requests
to restrict a~media resource to a~certain fragment.
Currently supported media fragment {\sc uri}s in the basic version
cover the temporal and the spatial dimension.
The temporal dimension denotes a~specific time range in the original media,
such as ``starting at second 10, continuing until second 20.''
The spatial dimension denotes a~specific range of pixels in the original media,
such as ``a~rectangle of size $ 100 \times 100 $
with its top-left at the coordinates $ (10, 10) $.''
Combinations of both dimensions are possible,
as is combining {\sc uri} fragment and query requests.

\noindent \textit{Digital Storytelling:}
Pizzi and Cavazza report in~\cite{pizzi2008debugging} on the development of
an authoring technology on top of an interactive storytelling system
that originated as a~debugging%
\footnote{Note, Pizzi and Cavazza use the term \emph{debugging} in the non-IT sense:
to check for redundancy, dead-ends, consistency, \emph{etc.} in authored stories}
tool for a~planning system.
Alexander and Levine define in~\cite{alexander2008storytelling}
the term \emph{Web~2.0 storytelling}, where people create \emph{microcontent}---%
small chunks of content, with each chunk conveying a~primary idea or concept---%
that gets combined with social media to form coherent stories.
We use Media Fragments {\sc uri}s to help raters understand
the results of an algorithm by converting dry software debugging data
to digital stories.

\section{Media Fragment Requirements}
\label{sec:media-fragment-requirements}

In the context of our research on media item deduplication,
a~media fragment identifier needs to be capable of expressing the following concepts.

\begin{enumerate}[i]
  \itemsep0em 
  \item Given a~rectangular media item with the dimensions $ width \times height $,
    express that in turn rectangular tiles
    of smaller dimensions are part of the original media item.
  \item Given detected faces at the granularity level of bounding rectangles,
    express that these bounding rectangles are within the dimensions
    of the original media item.
  \item Requirements \textit{i} and \textit{ii} need to be fulfilled for both
    types of media items, photos and videos;
    where in case of the latter, video subsegments of any length---%
    including video still frames---need to be supported.
\end{enumerate}

Media Fragment {\sc uri}~\cite{troncy2012mediafragments} as described in
the basic version of the specification supports all three requirements.
The \emph{temporal dimension} is denoted by the parameter name \texttt{t}
and specified as an interval with a~begin time and an end time.
Either one or both parameters may be omitted,
with the begin time defaulting to 0 seconds
and the end time defaulting to the duration of the source media item.
The interval is half-open: the begin time is considered part of the interval,
whereas the end time is considered to be the first time point
that is not part of the interval.
If only a~single value is present, it corresponds to the begin time,
except for when it is preceded by a~comma, which indicates the end time.
The temporal dimension is specified as Normal Play Time ({\sc npt},~\cite{schulzrinne1998realtime}).

The \emph{spatial dimension} selects an area of pixels from media items.
In the current version of the specification,
only rectangular selections are supported.
Rectangles can be specified as pixel coordinates or percentages.
Rectangle selection is denoted by the parameter name \texttt{xywh}.
The value is either \texttt{pixel:} or \texttt{percent:}
(defaulting to \texttt{pixel:}) and four comma-separated integers.
The integers denote $ x $, $ y $, $ width $, and $ height $ respectively,
with $ x = 0 $ and $ y = 0 $ being the top left corner of the media item.
If \texttt{percent:} is used,
$ x $ and $ width $ are interpreted as a~percentage
of the width of the original media item,
while $ y $ and $ height $ are interpreted as a~percentage
of the original height.

The intent of the Ontology for Media Resources~\cite{lee2012mediaontology}
by Lee \emph{et~al.} is to bridge the different descriptions of media resources
and to provide a~core set of descriptive properties.
Combined with Media Fragments {\sc uri},
this allows for making statements about media items and fragments thereof.
An example in RDF Turtle syntax~\cite{prudhommeaux2013turtle}
is given in \autoref{code:media-fragment}.

\begin{lstlisting}[caption={Description of two 10~sec long media fragments:
  \textit{(i)}~a~tile of dimensions $ 30 \times 40 $ pixels
  starting at pixel coordinates $ (0, 0) $
  that contains a~face; and
  \textit{(ii)}~a~tile of dimensions $ 10 \times 10 $ pixels
  starting at pixel coordinates $ (0, 0) $ of red color},
  label=code:media-fragment, float=b!]
@base <http://example.org/> .
@prefix ma: <http://www.w3.org/ns/ma-ont> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix db: <http://dbpedia.org/resource/> .
@prefix dbo: <http://dbpedia.org/ontology/> .
@prefix col: <http://purl.org/colors/rgb/> .

<video> a ma:MediaResource .

<video#t=,10&xywh=0,0,30,40> a ma:MediaFragment ;
                             foaf:depicts db:Face .
                             
<video#t=,10&xywh=0,0,10,10> a ma:MediaFragment ;
                             dbo:colour col:f00 .
\end{lstlisting}

\section{Media Item Deduplication Algorithm}
\label{sec:media-item-deduplication-algorithm}

\subsection{Algorithm Description}

Our near-duplicate media item clustering algorithm belongs to the family of
tile-wise histogram-based clustering algorithms.
As an additional semantic feature, the algorithm considers detected faces.
It is capable of deduplicating both media items of type video and photo.
In the case of video, frames at camera shot boundaries are used. % insert citation
For two media items to be clustered,
the following conditions have to be fulfilled.

\begin{description}
  \itemsep0em 
  \item[Cond.~1] Out of $m$ tiles of a~media item with $n$ tiles ($m \leq n$),
    at most $\textit{tiles\_threshold}$ tiles may differ not more than $\textit{similarity\_threshold}$
    from their counterpart tiles.
  \item[Cond.~2] The numbers $f_1$ and $f_2$ of detected faces in both media items
    have to be the same.
    We note that we do not \emph{recognize} faces, but only \emph{detect} them.
  \item[Cond.~3] If the colors of a~tile and its counterpart tile are within the
  $\textit{bw\_tolerance}$, these tiles are not considered
  and $\textit{tiles\_threshold}$ is decreased accordingly
  (we speak of $\textit{effective\_tiles\_threshold}$ in
  \autoref{sec:debugging-the-algorithm}).
\end{description}

The $\textit{bw\_tolerance}$ avoids media items to be clustered
when the particular tiles are too dark
(\emph{e.g.}, for the video borders in \autoref{fig:near-duplicate})
or too bright (\emph{e.g.}, for screenshots of Web pages or applications,
which frequently occur on social networks).
In order to illustrate the way the algorithm deduplicates media items,
\autoref{fig:algorithmdebug} shows a~debug view of the algorithm
for the two clustered media items related to the previous example around the
\emph{Backstreet Boys} music video from \autoref{fig:near-duplicate}.
Independent of the actual media item's aspect ratio,
the tile-wise comparison always happens based on a~potentially squeezed
square aspect ratio version.

\begin{figure}[b!]
  \centering
  \subfloat[From Facebook user]{
    \includegraphics[width=0.3\linewidth]{debug1.png}
  }                
  \subfloat[From Google+ user]{
    \includegraphics[width=0.3\linewidth]{debug2.png}
  }
  \caption{Debug view of the media item deduplication algorithm
    (as in the case of \autoref{fig:near-duplicate} no faces were detected,
    clustering happens based on tile similarity;
    black tiles not considered)}
  \label{fig:algorithmdebug}  
\end{figure}

\subsection{Debugging the Algorithm}
\label{sec:debugging-the-algorithm}

In this section, we consider the following debug scenarios
that during our previous experiments occurred most frequently.

\begin{description}
  \itemsep0em 
  \item[Clustering Consent]
    Two or more media items are clustered by the algorithm
    and the human rater agrees.
    The human rater wants to understand why they were clustered.
  \item[Clustering Dissent]
    Two or more media items are clustered by the algorithm,
    but the human rater thinks that they should not have been clustered.
    The human rater wants to understand why they were incorrectly clustered.
  \item[Non-Clustering Dissent]
    Two or more media items are not clustered by the algorithm,
    but the human rater thinks that they should have been clustered.
    The human rater wants to understand why they were not clustered.
\end{description}

In order to provide answers to the human rater's information needs above,
different levers of the algorithm's internals need to be debugged.
Is the $\textit{tiles\_threshold}$
(\emph{i.e.}, the number of tiles that may differ)
too high or too low?
Complementary to this, is the $\textit{similarity\_threshold}$
(\emph{i.e.}, the maximum amount two tiles may differ)
too high or too low (\textbf{Cond.~1})?
Are the number of detected faces $f_1$ and $f_2$ the same?
Are all faces correctly detected,
or should the face matching condition be temporarily disregarded,
\emph{e.g.}, with too tiny media items  (\textbf{Cond.~2})?
If the compared media items have very dark and/or very bright parts,
is the $\textit{bw\_tolerance}$ too high or too low (\textbf{Cond.~3})?

Consequently, the low-level debug output must include
the currently selected $\textit{tiles\_threshold}$ and
$\textit{similarity\_threshold}$
and how many tiles with these settings currently fulfill \textbf{Cond.~1}.
In addition to that, the debug output has to contain
the number of detected faces $f_1$ and $f_2$
in each media item, \emph{i.e.}, whether \textbf{Cond.~2} is fulfilled,
and the number of under the present $\textit{bw\_tolerance}$
not considered tiles, which implies fulfillment of \textbf{Cond.~3}
and potentially impacts \textbf{Cond.~1}
in form of the $\textit{effective\_tiles\_threshold}$.
The low-level debug output for the media items
from \autoref{fig:near-duplicate} and \autoref{fig:algorithmdebug}
looks as follows.

\begin{verbatim}
  Cond. 1) Similarity threshold: 15
  Cond. 1) Tiles threshold: 67
  Cond. 1) Similar tiles: 53
  Cond. 2) Faces left: 0. Faces right: 0
  Cond. 3) BW tolerance: 1
  Cond. 3) Not considered tiles: 23
  Cond. 3) Effective tiles threshold: 44
\end{verbatim}

While this low-level debug output is sufficient to respond to the polar question
(yes--no question) whether media items are clustered or not,
it does not help with the non-polar \emph{why} question
(the linguistic term is \emph{wh--question}).

\section{From Debug Output to Story}
\label{sec:from-debug-output-to-story}

In order for human raters to get answers to the wh--question
on \emph{why} media items are clustered,
we need to lift the low-level debug output
to a~high-level natural language story
for the previously defined debug scenarios
\textbf{Clustering Consent}, \textbf{Clustering Dissent},
and \textbf{Non-Clustering Dissent}.

We use the eSpeak~\cite{duddington2012espeak}
speech synthesizer developed by Jonathan Duddington
in a~JavaScript port made available by Alon Zakai.%
\footnote{Speak.js: \url{https://github.com/kripken/speak.js}}

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusions and Future Work}
\label{sec:conclusions-and-future-work}

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{icme2013template}

\end{document}
