%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Tell me why! Ain't nothin' but a mistake? Describing Media Item Differences with Media Fragments URI and Speech Synthesis  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Template for ICME-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass{article}

\usepackage{spconf,amsmath,epsfig}
\usepackage[utf8]{inputenc}
\usepackage{subfig}
\usepackage{enumerate}
\usepackage{setspace}

% autoref command
\usepackage[hyphens]{url}
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\def\subfigureautorefname{Subfigure}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=2em,belowskip=1em,abovecaptionskip=0.5em,belowcaptionskip=0.5em,
        framexbottommargin=-1em,basicstyle=\ttfamily\listingsize\selectfont}

\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Beginning of document  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}\sloppy

% for 1st, 2nd, etc. superscripting
\newcommand{\ts}{\textsuperscript}

% Title.
\title{Tell me why! Ain't nothin' but a~mistake? Describing Media Item Differences with Media Fragments URI and Speech Synthesis}

\twoauthors
  {Thomas Steiner}
	{Google Germany GmbH\\
	Hamburg, Germany\\
    \texttt{tomac@google.com}}
  {Raphaël Troncy}
	{EURECOM\\
    Sophia Antipolis, France\\
	\texttt{raphael.troncy@eurecom.fr}}

\maketitle

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}
We have developed a~tile-wise histogram-based media item deduplication algorithm with additional high-level semantic matching criteria that is tailored to photos and videos gathered from multiple social networks. In this paper, we investigate whether the Media Fragments {\sc uri} addressing scheme together with a~natural language generation framework realized through a~text--to--speech system provides a~feasible and practicable way to visually and audially describe the differences between media items of type photo and/or video, so that human-friendly debugging algorithm is made possible. A short screencast is available online at \url{http://youtu.be/DWqwEnhqTSc}.
\end{abstract}

\begin{keywords}
Media Fragments {\sc uri}, Media Fragments, Media Items, Deduplication, Social Networks
\end{keywords}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}
The music band \emph{Backstreet Boys}~({\sc bsb}) was formed in~1993 in Orlando,~FL and is still the best-selling boys band of all time. In~2013, the band will celebrate their 20\ts{th}~anniversary with a~new album and a~world tour. Reason enough for us to make them titular saint of this paper with their hit song \emph{I~Want It That Way} from the album \emph{Millennium}. While the spike of their career was in the late 90s, even today people still actively share
\footnote{{\sc bsb} shared on Google+: \url{http://bit.ly/backstreet-gplus}}, publish \footnote{{\sc bsb} published on Facebook: \url{http://bit.ly/backstreet-fb}} and follow the group on \emph{social networks}.

Social networks are at the heart of our research on event summarization, specifically deduplicating \emph{exact-} and \emph{near-duplicate} media items that are sometimes embedded in status messages referred to as \emph{microposts} on multiple social networks. In the context of our research, we define a~\emph{media item} as either a~photo (image) or a video that was \emph{publicly} shared or published on at least one social network. \autoref{fig:near-duplicate} shows an example where two users of the social networks Facebook and Google+ independently of each other share a~\emph{near-duplicate} media item in form of the music video \emph{Everybody} performed by the \emph{Backstreet Boys}. In order to detect, deduplicate, and cluster such occurrences of \emph{exact-} and \emph{near-duplicate} media items, we have implemented a~tile-wise histogram-based algorithm with additional high-level semantic matching criteria that was shown to work effectively and efficiently for several events.

\begin{figure}[b!]
  \centering
  \includegraphics[width=0.9\linewidth]{./backstreetboys.png}
  \caption{\emph{Near-duplicate} music video \emph{Everybody} by the \emph{Backstreet Boys} shared independently on Facebook and Google+}
  \label{fig:near-duplicate}
\end{figure}

During previous experiments on deduplicating event-related media items, we noticed that human raters wanted to know \emph{why}\footnote{Tell me why! Ain't nothin' but a~mistake?} particular media items are clustered as \emph{exact-} or \emph{near-duplicates}. In this paper, we investigate in how far
Media Fragments {\sc uri}~\cite{troncy2012mediafragments} combined with speech synthesis provide a~feasible way to tell human annotators why media items are clustered. As we deal with media items of type photo and/or video, we make simultaneous use of two types of media fragments dimensions: the temporal dimension and the spatial dimension.

The remainder of this paper is structured as follows. In \autoref{sec:related-work}, we report on related work on media fragments, digital storytelling, and natural language generation. In \autoref{sec:media-fragment-requirements}, we describe our requirements on media fragments identifiers. In \autoref{sec:media-item-deduplication-algorithm}, we detail how the media item deduplication algorithm works and show low-level debugging approaches to check
\emph{if} or \emph{if not} media items are clustered. In \autoref{sec:from-debug-output-to-story}, we elaborate on how this low-level debug output gets lifted
to natural language stories for human raters to understand \emph{why} or \emph{why not} media items are clustered. We evaluate our approach in \autoref{sec:evaluation}. We conclude and give an outlook on future work in \autoref{sec:conclusions-and-future-work}.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  2. Related Work  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related-work}
\textbf{Media Fragments.} There are many online video hosting platforms that have some sort of media fragments support. In the following, we present two representative ones. The video hosting platform YouTube\footnote{YouTube: \url{http://www.youtube.com/}} allows for deep-linking into videos via a~proprietary {\sc url} parameter \texttt{t}, whose value has to match the regular expression \texttt{\textbackslash d+m\textbackslash d+s} (for minutes and seconds),
as documented in~\cite{youtube2008link}. Dailymotion\footnote{Dailymotion: \url{http://www.dailymotion.com/}} has similar {\sc url} parameters \texttt{start} and \texttt{end}, whose value has to match the regular expression \texttt{\textbackslash d+} (for seconds). The {\sc css} Backgrounds and Borders Module Level~3 specification~\cite{bos2012css3} defines the \texttt{background-size} property that can be used to crop media items visually and thus create the illusion of a~spatial media fragment when combined with a wrapping element. Media Fragments {\sc uri}~\cite{troncy2012mediafragments} specifies a~syntax for constructing media fragments {\sc uri}s and explains how to handle them when used over the {\sc http} protocol~\cite{fielding1999http}. The syntax is based on the specification of particular name-value pairs that can be used in {\sc uri} query strings and {\sc uri} fragment identifiers to restrict a~media resource to a~certain fragment. The temporal and spatial dimensions are currently supported in the basic version of Media Fragments {\sc uri}s. Combinations of dimensions are also possible.

\noindent \textbf{Digital Storytelling.} Pizzi and Cavazza report in~\cite{pizzi2008debugging} on the development of an authoring technology on top of an interactive storytelling system that originated as a~debugging\footnote{Pizzi and Cavazza use the term \emph{debugging} in the non-IT sense: to check for redundancy, dead-ends, consistency, \emph{etc.} in authored stories} tool for a~planning system. Alexander and Levine define in~\cite{alexander2008storytelling}
the term \emph{Web~2.0 storytelling}, where people create \emph{microcontent}--%
small chunks of content, with each chunk conveying a~primary idea or concept--%
that gets combined with social media to form coherent stories. We use Media Fragments {\sc uri}s to help human annotators to understand the results of an algorithm by converting dry software debugging data to digital stories.

\noindent \textbf{Natural Language Generation.} Natural language generation is the natural language processing task of generating natural language from a~machine representation system. This field is covered in great detail by Reiter and Dale in~\cite{reiter2000building}. They divide the task into three stages: document planning, microplanning, and realization. \emph{Document planning} determines the content and structure of a~document. \emph{Microplanning} decides which words, syntactic structures, \emph{etc.} will be used to communicate the chosen content and structure. \emph{Realization} maps the abstract representations
used by microplanning into actual text.

\begin{lstlisting}[caption={Description of two 10~sec long media fragments:
  \textit{(i)}~a~tile of dimensions $ 30 \times 40 $ pixels
  starting at pixel coordinates $ (0, 0) $
  that contains a~face; and
  \textit{(ii)}~a~tile of dimensions $ 10 \times 10 $ pixels
  starting at pixel coordinates $ (0, 0) $ of red color},
  label=code:media-fragment, float=b!]
@base <http://example.org/> .
@prefix ma: <http://www.w3.org/ns/ma-ont> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix db: <http://dbpedia.org/resource/> .
@prefix dbo: <http://dbpedia.org/ontology/> .
@prefix col: <http://purl.org/colors/rgb/> .

<video> a ma:MediaResource .
<video#t=,10&xywh=0,0,30,40> a ma:MediaFragment ;
                             foaf:depicts db:Face .
<video#t=,10&xywh=0,0,10,10> a ma:MediaFragment ;
                             dbo:colour col:f00 .
\end{lstlisting}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  3. Media Fragments Requirements  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Media Fragments Requirements}
\label{sec:media-fragment-requirements}

In the context of our research on media item deduplication and clustering, media fragments identifiers need to be capable of expressing the following concepts.
\begin{enumerate}[i]
  \itemsep0em
  \item Given a~rectangular media item with the dimensions $ width \times height $, express that in turn rectangular tiles of smaller dimensions are part of the original media item.
  \item Given detected faces at the granularity level of bounding rectangles, express that these bounding rectangles are within the dimensions of the original media item and that each bounding rectangle contains a~face.
  \item Requirements \textit{i} and \textit{ii} need to be fulfilled for both types of media items, photos and videos; In case of the latter, video subsegments of any length--including video still frames---need to be supported.
\end{enumerate}

Media Fragments {\sc uri}~\cite{troncy2012mediafragments} as described in the basic version of the specification supports all three requirements. The \emph{temporal dimension} is denoted by the parameter name \texttt{t} and specified as an interval with a~begin time and an end time. Either one or both parameters may be omitted, with the begin time defaulting to 0 seconds and the end time defaulting to the duration of the source media item. The interval is half-open: the begin time is considered part of the interval, whereas the end time is considered to be the first time point that is not part of the interval.
If only a~single value is present, it corresponds to the begin time, except for when it is preceded by a~comma, which indicates the end time. The temporal dimension is specified as Normal Play Time ({\sc npt},~\cite{schulzrinne1998realtime}).

The \emph{spatial dimension} selects an area of pixels from media items. In the current version of the specification, only rectangular selections are supported. Rectangles can be specified as pixel coordinates or percentages. Rectangle selection is denoted by the parameter name \texttt{xywh}. The value is either \texttt{pixel:} or \texttt{percent:} (defaulting to \texttt{pixel:}) and four comma-separated integers. The integers denote $x$, $y$, $width$ and $height$ respectively, with $x = 0$ and $y = 0$ being the top left corner of the media item. If \texttt{percent:} is used,\linebreak
$x$ and $width$ are interpreted as a~percentage of the width of the original media item, while $y$ and $height$ are interpreted as a~percentage of the original height.

The intent of the Ontology for Media Resources~\cite{lee2012mediaontology} by Lee \emph{et~al.} is to bridge different description methods of media resources
and to provide a~core set of descriptive properties. Combined with Media Fragments {\sc uri}, this allows for making statements about media items and fragments thereof. An example in RDF Turtle syntax~\cite{prudhommeaux2013turtle} is given in \autoref{code:media-fragment}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  4. Media Item Deduplication Algorithm  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Media Item Deduplication Algorithm}
\label{sec:media-item-deduplication-algorithm}

\subsection{Algorithm Description}
The deduplication algorithm described in this paper belongs to the family of tile-wise histogram-based clustering algorithms. As an additional semantic feature, the algorithm considers detected faces. It is capable of deduplicating media items of type video and/or photo. In the case of video, frames at camera shot boundaries are used. A~camera shot in video production and filmmaking is a~series of frames that runs for an uninterrupted period of time. For two media items to be clustered, the following three clustering conditions have to be fulfilled.
\begin{description}
  \itemsep0em
  \item[Cond.~1] Out of $m$ tiles of a~media item with $n$ tiles ($m \leq n$), the average color of at most $\textit{tiles\_threshold}$ tiles may differ not more than $\textit{similarity\_threshold}$ from their counterpart tiles.
  \item[Cond.~2] The numbers $f_1$ and $f_2$ of detected faces in both media items have to be the same. We note that the algorithm does not \emph{recognize} faces, but only \emph{detects} them.
  \item[Cond.~3] If the average colors of a~tile and its counterpart tile are within the black--and--white tolerance $\textit{bw\_tolerance}$, these tiles are not considered and $\textit{tiles\_threshold}$ is decreased accordingly (we talk about $\textit{effective\_tiles\_threshold}$ in \autoref{sec:debugging-the-algorithm}).
\end{description}

The black--and--white tolerance $\textit{bw\_tolerance}$ avoids media items to be clustered when the particular tiles are too dark (\emph{e.g.}, for the video borders in \autoref{fig:near-duplicate}) or too bright (\emph{e.g.}, for screenshots of Web pages or applications, which frequently appear on social networks). In order to illustrate the way the algorithm deduplicates media items, \autoref{fig:algorithmdebug} shows a~debug view of the algorithm for the two clustered media items related to the previous example around the \emph{Backstreet Boys} music video. Independent of the actual media item's aspect ratio, the tile-wise comparison always happens based on a~potentially squeezed square aspect ratio version.

\begin{figure}[t!]
  \centering
  \subfloat[From Facebook user]{
    \includegraphics[width=0.35\linewidth]{debug1.png}
  }
  \subfloat[From Google+ user]{
    \includegraphics[width=0.35\linewidth]{debug2.png}
  }
  \caption{Debug view of the media item deduplication algorithm: since no faces are detected on \autoref{fig:near-duplicate}, the clustering is based on tile similarity; black tiles are not considered due to the chosen black--and--white tolerance)}
  \label{fig:algorithmdebug}
\end{figure}

\subsection{Debugging the Algorithm}
\label{sec:debugging-the-algorithm}
In this section, we consider the following three debug scenarios that occurred most frequently during our previous experiments with human raters. They correspond to situations where, given a~set of deduplicated and clustered media items, a human annotator wanted to understand the specific details leading to the decisions taken by the algorithm that they were unsure about.
\begin{description}
  \itemsep0em
  \item[Clustering Consent.] Two or more media items are clustered by the algorithm and the human rater agrees. The human rater wants to understand why they were clustered.
  \item[Clustering Dissent.] Two or more media items are clustered by the algorithm, but the human rater thinks that they should not have been clustered. The human rater wants to understand why they were incorrectly clustered.
  \item[Non-Clustering Dissent.] Two or more media items are not clustered by the algorithm, but the human rater thinks that they should have been clustered. The human rater wants to understand why they were not clustered.
\end{description}

In order to provide answers to these human raters' information needs, different levels of the algorithm's internals have to be debugged. Is the $\textit{tiles\_threshold}$ (\emph{i.e.}, the number of tiles that may differ) too high or too low? Complementary to this, is the $\textit{similarity\_threshold}$ (\emph{i.e.}, the maximum amount two tiles may differ) too high or too low (\textbf{Cond.~1})? Are the number of detected faces $f_1$ and $f_2$ the same? Are all faces correctly detected, or should the face matching condition be temporarily disregarded, \emph{e.g.}, with too tiny media items, where faces fail to be detected (\textbf{Cond.~2})? If the media items to be compared have very dark and/or very bright parts, is the $\textit{bw\_tolerance}$ too high or too low (\textbf{Cond.~3})?

\subsection{Low-Level Debug Output}
\label{sec:low-level-debug-output}
As a~consequence of the previous subsection, the low-level debug output must include the currently selected $\textit{tiles\_threshold}$ and $\textit{similarity\_threshold}$ and how many tiles with the present algorithm settings currently fulfill \textbf{Cond.~1}. In addition to that, the debug output has to contain the number of detected faces $f_1$ and $f_2$ in each media item, \emph{i.e.}, whether \textbf{Cond.~2} is fulfilled, as well as the number of
not considered tiles (according to $\textit{bw\_tolerance}$) which implies fulfillment of \textbf{Cond.~3} and potentially impacts \textbf{Cond.~1} in form of the $\textit{effective\_tiles\_threshold}$. As an example, the low-level debug output for the media items from the example of the \emph{Backstreet Boys} media items for the music video \emph{Everybody} reads:
\begin{verbatim}
- Similarity threshold: 15 (Cond. 1)
- Tiles threshold: 67 (Cond. 1)
- Similar tiles: 52 (Cond. 1)
- Faces left: 0. Faces right: 0 (Cond. 2)
- BW tolerance: 1 (Cond. 3)
- Not considered tiles: 22 (Cond. 3)
- Effective tiles threshold: 45 (Cond. 3)
\end{verbatim}

While this low-level debug output is sufficient to answer to the polar question whether media items are clustered at all or not, it does not help with the non-polar \emph{why} question. In the following section, we show how this low-level debug output can be lifted.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  5. From Debug Output to Story  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{From Debug Output to Story}
\label{sec:from-debug-output-to-story}
In order for human raters to get answers to the question on \emph{why} media items are clustered, we need to lift the low-level debug output to a~high-level natural language story for the previously defined debug scenarios \textbf{Clustering Consent}, \textbf{Clustering Dissent}, and \textbf{Non-Clustering Dissent}. This results in a~natural language generation task, whose three stages according to Reiter's and Dale's architecture~\cite{reiter2000building} will be detailed in the following subsections.

\subsection{Generating Natural Language}

\subsubsection{Document Planning}
In our context, the document is a~set of low-level debug data as illustrated in \autoref{sec:low-level-debug-output}. The natural language generation task is thus manageable. We need to convey the $\textit{tiles\_threshold}$ and $\textit{similarity\_threshold}$ currently selected, the number of detected faces $f_1$ and $f_2$ in each media item and the number of tiles not considered given the $\textit{bw\_tolerance}$ parameter.

\subsubsection{Microplanning}
The microplanning task is driven by the debug scenarios described previously. Initially, we need to decide on a~matching condition aspect of the algorithm that will be first highlighted. Typically, this will be the overall tiles statistics. Afterwards, we need to elaborate on secondary matching conditions such as detected faces and black--and--white tolerance. The grammatical number (plural or singular) needs to be taken into account when statements about tile(s) or face(s) are planned. Some values, \emph{e.g.}, the percentage of matching tiles, are calculated. The microplanner needs to decide when exactness (\emph{e.g.}, \textit{``99\% of all tiles''}), and when approximation of calculated values (\emph{e.g.}, \textit{``roughly 50\%''}) better suits the human evaluators' information needs. Neutral non-judgmental statements (\emph{e.g.}, \textit{``45 tiles''}) and biased judgmental statements (\emph{e.g.}, \textit{``not a~single one [tile]''}) need to be carefully balanced. Finally, in the interest of a~more naturally sounding phrase composition, the microplanner needs to be aware of contrasting juxtaposition (\emph{e.g.}, \textit{``Both the left and the right media item contain 1 detected face.''} \emph{vs.} \textit{``The left media item contains no detected faces, while the right media item contains 1 detected face.''}).

\subsubsection{Realization}
We show examples of sentences that are actually generated for the three different debug scenarios (Quotes~1--3). For the sake of completeness, we provide one additional example (Quote~4) for the debug scenario \textbf{Non-Clustering Consent}. The running example of the \emph{Backstreet Boys} media items for the music video \emph{Everybody} is represented by Quote~1.

\begin{description}
  \itemsep0em
  \item[Clustering Consent] (Quote~1). \textit{``The two media items are near-duplicates. Out of overall 100 tiles, 52 from the minimum required 45 tiles
      were similar enough to be clustered. This corresponds to 52 percent of all tiles. However, 22 tiles were not considered, as they are either too bright or too dark, which is a common source of clustering issues. Neither the left, nor the right media item contain detected faces.''}
  \item[Clustering Dissent] (Quote~2). \textit{``The two media items are near-duplicates. Out of overall 100 tiles, 41 from the minimum required 41 tiles were similar enough to be clustered. This corresponds to 41 percent of all tiles. However, 26 tiles were not considered, as they are either too bright
      or too dark, which is a common source of clustering issues. Neither the left, nor the right media item contain detected faces.''}
  \item[Non-Clustering Dissent] (Quote~3). \textit{``The two media items are different. Out of overall 100 tiles, only 8 from the minimum required 67 tiles
      were similar enough to be clustered. This corresponds to 8 percent of all tiles. The left media item contains 2 detected faces, while the right media item contains 1 detected face.''}
  \item[(Non-Clustering Consent)] (Quote~4). \textit{``The two media items are different. Out of overall 100 tiles, not a single one was similar enough to be clustered. Neither the left, nor the right media item contain detected faces.''}
\end{description}

\subsection{Technical Implementation}

\subsubsection{Text--to--Speech}
The generated texts are converted to speech using a~text--to--speech system. We use the eSpeak~\cite{duddington2012espeak} speech synthesizer that was originally developed by Jonathan Duddington in a~JavaScript port made available by Alon Zakai.\footnote{Speak.js: \url{https://github.com/kripken/speak.js}}. This speech synthesizer uses the formant synthesis method, which allows many languages to be provided in a~small size. Rather than using human speech samples at runtime, the synthesized speech output is created using additive synthesis and an acoustic model, where parameters such as fundamental frequency, voicing, and noise levels
are varied over time to create a~waveform of artificial speech. The speech is clear and can be used at high speeds. However, it is not as natural or smooth as larger synthesizers that are based on speech recordings.

\subsubsection{Visual Media Fragments Highlighting}
We treat and address each tile of a~media item as a~spatial media fragment. \autoref{fig:similar-different} shows, in a~grid-like structure, similar, different, and tiles not considered from the \emph{Backstreet Boys} media items for the \emph{Everybody} music video. While the speech synthesizer reads the generated text, the corresponding tiles (\emph{e.g.}, the matching tiles or the not considered tiles due to the black--and--white tolerance) are interactively visually highlighted to support the human evaluators' understanding. Spatial Media Fragments {\sc uri}s are currently not implemented in any of the existing Web browsers~\cite{weinberg2013polyfill}. In order to nonetheless support spatial media fragments, we use a~so-called JavaScript polyfill for Media Fragments {\sc uri} made available by Thomas Steiner\footnote{xywh.js: \url{https://github.com/tomayac/xywh.js}}. In Web development, a~polyfill is downloadable code that provides facilities by emulating potential future features or {\sc api}s that are not built-in to a~Web browser~\cite{sharp2010polyfill}. Steiner's polyfill--in contrast to an additional spatial Media Fragments {\sc uri} polyfill implementation~\cite{weinberg2013polyfill} by Fabrice Weinberg-- supports more browsers and both image \emph{and} video.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.75\linewidth]{./similar-different.png}
  \caption{Similar (upper row) and different (lower row) corresponding tile pairs for the media items from a~Facebook (left column) and a~Google+ user (right column); checkered tiles are not considered due to the black--and--white tolerance}
  \label{fig:similar-different}
\end{figure}

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.75\linewidth]{./tile-highlight.png}
  \caption{Red highlighted tiles not considered, while the text--to--speech system reads the corresponding text fragment \textit{``However, 22 tiles were not considered, as they are either too bright or too dark, which is a common source of clustering issues.''}}
  \label{fig:tile-highlight}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%
%%%  6. Evaluation  %%%
%%%%%%%%%%%%%%%%%%%%%%%

\vspace{-0.5cm}
\section{Evaluation}
\label{sec:evaluation}
For the evaluation of natural language generating systems, there are three basic techniques. First, the \emph{task-based} or \emph{extrinsic} evaluation,
where the generated text is given to a~person who evaluates how well it helps with performing a~given task~\cite{portet2009nlg}. Second, there are \emph{automatic metrics} such as {\sc bleu}~\cite{papineni2002bleu}, where the generated text is compared to texts written by people based on the same input data. Finally, there are \emph{human ratings}, where the generated text is given to a~person who is asked to rate the quality and usefulness of the text.

For our evaluation, we chose the third approach of human ratings, as we do not evaluate the natural language generating system in isolation but in \emph{combination with a~visual representation} that makes use of spatial Media Fragments {\sc uri}s (\autoref{fig:similar-different} and \autoref{fig:tile-highlight}). Evaluating subjective data like the quality and usefulness of an auto-generated textual, visual, and audial explanation of the results of a~deduplication algorithm is a~challenging task. For different users, there may be different emphasis. A~common subjective evaluation technique
is the Mean~Opinion Score~({\sc mos},~\cite{itu1998mos}), used for decades in telephony networks to obtain the human user's view of the quality of a~network.
Recently, {\sc mos} has also found wider usage in the multimedia community. Therefore, we conducted a~set of standard subjective tests, where users rate the perceived quality of test samples with scores from 1 (worst) to 5 (best). The actual {\sc mos} is then the arithmetic mean of all individual scores. In the context of this research, we have conducted {\sc mos} test sessions with five external human raters. We generated artificially modified deduplicated media item sets around media items about the \emph{Backstreet Boys} that were shared on social networks during the time of writing. These media item sets were curated by yet another independent two external persons, assisted by a~previously developed software system that implements the deduplication algorithm described in this paper. We asked the two persons to provoke dissent and consent clustering situations for the five human raters, \emph{i.e.}, obviously correct clustering (\textbf{Clustering Consent}), obviously incorrect clustering (\textbf{Clustering Dissent}), and obviously incorrect non-clustering (\textbf{Non-Clustering Dissent}). We then asked the five human raters to have the system automatically explain the algorithm results to them as described in \autoref{sec:from-debug-output-to-story}. The raters gave {\sc mos} scores ranging from $2$ to $5$, with the overall average values as follows: \textbf{Clustering Consent}: $4.3$, \textbf{Clustering Dissent}: $3.3$, and \textbf{Non-Clustering Dissent}: $4.1$. The human raters appreciated the parallel explanation approach, where the visual and the audial parts synchronously described what the algorithm was doing. They uttered that the not considered tiles (due to the black--and--white tolerance) as well as erroneously not detected faces were sources of error in the algorithm that they easily understood thanks to the human language description. They sometimes wished for more diversification in the generated texts. Without exception, they liked the system and encouraged future development.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  7. Conclusions and Future Work  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions and Future Work}
\label{sec:conclusions-and-future-work}
We have successfully demonstrated the feasibility of making the task of debugging a~complex algorithm human-friendly, by means of a~combined visual and audial approach. We have used Media Fragments {\sc uri} together with a~natural language generation framework realized through a~speech synthesizer to visually and audially describe media item differences. Future work will focus on generalizing the approach. Many algorithms need expert evaluators for fine-tuning their settings, simply because the debug output is unaccessible to untrained raters. With this work, we have contributed a~promising proof of concept, which, through the involvement of non-domain expert raters, has helped us greatly improve our deduplication algorithm's default parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Acknowledgments  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Acknowledgments}
This work was partially supported by the European Union's 7th Framework Programme via the project LinkedTV (GA 287911).

%%%%%%%%%%%%%%%%%%%%%%
%%%  Bibliography  %%%
%%%%%%%%%%%%%%%%%%%%%%

% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\begin{spacing}{0.934}
\bibliographystyle{IEEEbib}
\bibliography{icme2013}
\end{spacing}

\end{document}
