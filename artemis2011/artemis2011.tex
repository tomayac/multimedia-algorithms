\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{listings} 
\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}


% \iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ificcvfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Enabling On-the-Fly Video Scene Detection on YouTube\\ and Crowd-Sourcing In-Video Hot Spot Detection}

\author{Thomas Steiner\\
Google Germany GmbH\\
ABC Str. 19\\
{\tt\small tomac@google.com}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Ruben Verborgh, Rik Van de Walle \\
Ghent University -- IBBT, ELIS -- Multimedia Lab\\
Gaston Crommenlaan 8 bus 201, B-9050 Ledeberg-Ghent, Belgium\\
{\small \{ruben.verborgh, rik.vandewalle\}@ugent.be}
}

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Video scene detection is the CPU-intense task of splitting a video into continuous scenes, with hard or soft cuts as the boundaries. In this paper, we present a client-side on-the-fly approach to this challenge based on modern HTML5-enabled Web APIs. We show how using browser extensions video scene detection can be seamlessly embedded into video platforms like YouTube. Given a set of scenes for a video, we further explain how these scenes can serve to crowd-source interesting scene detection, in the following referred to as hot spot detection. We evaluate our approach based on a set of manually annotated videos, both for scenes and for hot spots.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Official statistics~\cite{youtube:stats} from YouTube~\cite{youtube}, one of the biggest online video platforms owned by Google, state that more than 13 million hours of video were uploaded during 2010, and that 35 hours of video are uploaded every single minute. Given this huge amount of video content, it becomes evident that in order to retrieve the few needles from the giant haystack advanced search techniques are necessary. Closed captions allow for keyword-based in-video search, a feature announced in~\cite{googlevideo}. Searching for a phrase like ``that's a tremendous gift", a caption from Randy Pausch's famous last lecture titled \emph{Achieving Your Childhood Dreams}~\cite{pausch}, indeed reveals a link to that lecture on YouTube. If no closed captions are available, nor can be automatically generated~\cite{youtubecaptions}, keyword-based search is still available over tags, video descriptions, and titles. Presented with a potentially long list of results, preview thumbnails based on video still frames help users decide on the most promising result. YouTube lets video owners choose one out of three automatically suggested thumbnails. Until December 2008, the mechanism for the generation of such thumbnails has been simply the so-called 25/50/75 rule~\cite{youtuberule}, referring to points in the video index. This led to creative abuse of the feature to artificially increase a video's click-through rate. Currently, YouTube uses an unpublished computer vision-based algorithm for the generation of smart thumbnails on YouTube~\cite{googleresearch}.

In this paper, we introduce on-the-fly scene detection for YouTube videos as a third means besides keyword-based search and thumbnail preview for deciding on a video from the haystack. As a user starts watching a video, we detect scenes in the video by visually analyzing its content. We do this with the help of a browser extension, i.e., the whole process runs dynamically on the client-side. We therefore use modern HTML5~\cite{w3c_html5} JavaScript APIs of the HTML \texttt{video} and {canvas} elements. As soon as the scenes have been detected, we offer the user the choice to quickly jump into a specific scene by clicking on the representative still frame. Figure~\ref{screenshot} shows the seamless integration of the detected scenes into the YouTube website. We refer to such scenes of interest as \emph{hot spots}. Clicks on hot spots are tracked using a standard Web analytics service. This allows us to crowd-source hot-spot detection and suggest more accurate entry points especially for long videos. The main contributions of this paper are the browser extension itself, the hot spot-enabled possibility for more accurate video thumbnail selection, improved video navigability by scene access, and the potential combination with text-based semantic enrichment of metadata.

The remainder of this paper is structured as follows: Section~\ref{sec:related-work} presents related work in the fields of shot boundary detection and semantic enrichment of user-generated metadata on YouTube, Section~\ref{sec:details-of-algo} explains our scene detection algorithm in detail, Section~\ref{sec:implementation} outlines the implementation details of our browser extension, Section~\ref{sec:evaluation} shows an evaluation of first preliminary results. Finally, Section~\ref{sec:future-work-conclusion} gives an outlook on future work and ends the paper with a conclusion.

\begin{figure*}
\begin{center}
   \includegraphics[width=0.8\linewidth]{./resources/screenshot.png}
\end{center}
   \caption{Screenshot of the YouTube scene detection browser extension.}
\label{fig:screenshot}
\end{figure*}

\section{Related Work} \label{sec:related-work}
We consider related work in the strictly related field of shot boundary detection and in the loosely related field of semantic enrichment of user-generated metadata on YouTube.

\subsection{Shot boundary detection}
Video fragments consist of shots, which are sequences of consecutive frames from a single viewpoint, representing an continuous action in time and space. The topic of shot boundary detection has already been described extensively in literature. While some specific issues still remain (notably gradual transitions and false positives due to large movement or illumination changes), the problem is considered resolved for many cases~\cite{Hanjalic2002, Yuan2007}. Below, we present an overview of several well-known categories of techniques.

Pixel comparison methods~\cite{Hampapur1994, Zhang1993} construct a discontinuity metric based on differences in color or intensity values of corresponding pixels in successive frames. This dependency on spatial location makes this technique very sensitive to (even global) motion. Various improvements have been suggested, such as prefiltering frames~\cite{Zhang1995}, but pixel-by-pixel comparison methods proved inferior in the end and have steered research to other directions.

A related method is histogram analysis~\cite{Smeaton1999}, where changes in frame histograms are used to justify the boundaries of shots. Their insensitivity to spatial information within a frame makes histograms less prone to partial and global movements in a scene. We can argue as a drawback that even visually very dissimilar frames can have similar overall histograms. For example, different shots in the same scene can be difficult to distinguish because of similar color information.

As a compromise, a third group of methods consisted of a trade-off between the above two techniques~\cite{Ahmed1999}. Different histograms of several, non-overlapping blocks are calculated for each frame, thereby categorizing different regions of the image with its own color-based, space-invariant fingerprint. The results are promising while computational complexity is kept to a minimum, which is why we have chosen a variation on this approach in this paper.

Other approaches to shot boundary detection include the comparison of mean and standard deviations of frame intensities~\cite{Lienhart1999}. Detection using other features such as edges~\cite{Zabih1995} and motion~\cite{Bouthemy1997} have also been proposed. However, Gargi \emph{et~al.}\ have shown that these more complex methods do not necessarily outperform histogram-based approaches~\cite{Gargi2000}. A detailed overview and comparison can be found in Yuan \emph{et~al.}~\cite{Yuan2007}. 

Another interesting track is shot boundary detection in the compressed domain~\cite{Yeo1995}, which is especially relevant for online video, as compression plays a major part herein. Unfortunately, even though this article focuses on online video, client-side manipulation of the raw, compressed stream would introduce a lot of computational overhead and make our implementation codec-dependent.

\subsection{Semantic Enrichment of User-Generated Metadata on YouTube}
In \cite{Choudhury:YouTube}, Choudhury et al. describe a framework for the semantic enrichment, ranking, and integration of Web video tags using Semantic Web technologies. In order to enrich the oftentimes sparse user-generated tag space, they use metadata like the recording time and location, or the video title and video description, but also social features such as playlists that a video appears in and related videos. Next, they rank the tags by their co-occurrence and in a final step interlink with DBpedia~\cite{Bizer:DBpedia}. While the semantic enrichment of YouTube metadata is of little help for the actual detection of scenes in a video, however, it still can help to visually ``iconify" them. For example, if it is known that a certain scene is about the DBpedia concept of the company Apple, Inc., this scene can then be visually represented by an icon in form of the Apple logo, in addition to the scene still frames.

In~\cite{semwebvid}, Steiner and Hausenblas describe a Web application that allows for the automatic generation of Resource Description Framework (RDF) video descriptions based on user-generated metadata such as title, description, tags, and closed captions. They enrich this textual information via multiple Natural Language Processing Web services in parallel in order to extract named entities. The detected named entities are interlinked with DBpedia concepts. While thanks to the closed captions their detected named entities are explicitly anchored to a point in the video, with our scene detection framework we can instead anchor named entities to scenes, which is context-wise oftentimes the better option.

Finally, Gao et al. propose in~\cite{Gao:2009} a theme-based thumbnail selection algorithm that explicitly models the visual characteristics of the underlying semantic video theme. This semantic model is constructed by finding the common features of relevant visual samples, which are obtained by querying a visual database with keywords associated with the video. Synergies between the approaches~\cite{Choudhury:YouTube,Gao:2009,semwebvid} and our current approach of scene detection with crowd-sourced hot spot detection are possible.

\section{Scene Detection Algorithm} \label{sec:details-of-algo}
In this Section, we discuss our scene detection algorithm, which falls in the category of histogram-based algorithms.  It is driven by the common observation that different video still frames can have the same histogram. In order to minimize the probability of this to happen, we split our video frames in freely configurable rows and columns, hence, lay over a grid of tiles over the frames. The user interface currently allows for anything from $1 * 1$ to $20 * 20$ layouts. For each step we examine a frame $n$ and its direct predecessor frame $n - 1$.

Apart from the obvious average histogram value per-tile distance, in our frame distance function we further consider an also freely configurable number of to-be-examined most different or similar tiles. This is driven by the observation that different parts of the video have different intensities of color changes, dependent on the movements from frame to frame. The idea is thus to increase the influence of movements in the frame distance function, and to decrease the influence of static parts. This can be best illustrated with the example given in Figure~\ref{fig:algorithm}. The anchorman is framed by two news tickers, one above, and one below his head. There are many red boxes, i.e., movements, on the tiles with the news tickers. In contrast, the top-upper gray background of the studio wall is entirely static, in consequence there are many green boxes. Besides from the anchorman's movements which result in red boxes around his face, there are many screens with live video to be seen behind the anchorman, which explains the remaining red boxes. For this example, we used a grid layout of $20x20$ (which implies $numTiles = rows * columns$), and a number $nDiffSim$ of most different or similar tiles of $133$ (which corresponds to $rows * columns / 3$). In our frame distance function we apply boosting and limiting factors to the most different and most similar tiles respectively. In our example we worked with a $boostingFactor$ of $2$, which doubles the impact of the most different tiles, and a $limitingFactor$ of $0.5$, which of course halves the impact of the most similar tiles. The complete algorithm can be seen in Listing~\ref{code:algorithm}.

\begin{figure}
\begin{center}
   \includegraphics[width=1.0\linewidth]{./resources/algorithm.png}
\end{center}
   \caption{Debug view of the scene detection process. Red boxes highlight tiles with the most, green boxes with the least differences to the previous frame.}
\label{fig:algorithm}
\end{figure}

\begin{lstlisting}[caption=Pseudocode for our scene detection algorithm., label=code:algorithm, float=t, frame=single]
dist = []
for i = 0; i < numTiles; i++
  frameN = getFrame(i)
  frameN-1 = getFrame(i - 1)
  dist[i] = histoDist(frameN, frameN-1)
  
mostDiffTiles = []
mostSimTiles = []
mostDiffTiles =
    getMostDiffTiles(dist, nDiffSim)
mostSimTiles =
    getMostSimTiles(dist, nDiffSim)

for i = 0; i < numTiles; i++
  currentTile = getTile(i)
  fact = 1
  if tile in mostDiffTiles
    fact = boostingFactor
  if tile in mostSimTiles
    fact = limitingFactor
  frameN.avgHisto(currentTile) =
    frameN.avgHisto(currentTile) * fact
    
frameN.avgHisto =
    getAvgHisto(frameN)    
\end{lstlisting}

We have made some basic statistic examinations of the frame distance data and experimentally found out that while the average frame distance defined as $$avgDist = \frac{1}{numTiles}\sum_{n=1}^{numTiles}frame_{n}.avgHisto$$ is very intuitive to human beings, far more value lies in the standard deviation, based on the definition of the average distance before $$\sqrt{\frac{1}{numTiles}\sum_{n=1}^{numTiles}(frame_{n}.avgHisto - avgDist)^{2}}$$ We can actually directly use the value of the standard deviation as a scene splitting threshold value to come to very accurate scene splitting results. We found the boosting and limiting factors to have overall a positive scene splitting quality impact with more lively videos, and a negative quality impact with more monotone videos. The results are best if after changing either the boosting or the limiting factor the scene splitting threshold gets adapted to the new standard deviation.

\section{Implementation Details} \label{sec:implementation}

\section{Evaluation} \label{sec:evaluation}

\section{Future Work and Conclusion} \label{sec:future-work-conclusion}

{\small
\bibliographystyle{ieee}
\bibliography{artemis2011}
}


\end{document}
