\documentclass{acm_proc_article-sp}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[activate=compatibility]{microtype}

% autoref command
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black, draft]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\def\subfloatautorefname{Subfigure}

\usepackage[lofdepth,lotdepth]{subfig}

\usepackage{enumitem}

\usepackage{mathtools}

\usepackage{eurosym}

\usepackage{fmtcount}

% give emph a normal fontsize
\let\oldemph\emph
\renewcommand{\emph}[1]{\oldemph{\fontsize{9}{9}\selectfont #1}}

% more readable footnote layout
\renewcommand{\footnotesize}{\fontsize{8pt}{10pt}}
\setlength{\footnotesep}{.5cm}

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}
\newenvironment{Todo}{\color{red}{\par{\bf TODO}}\everypar={\color{red}}{}}{}

% superscript for 1st, 2nd, etc.
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}
\newcommand{\st}[0]{\superscript{st}}
\newcommand{\nd}[0]{\superscript{nd}}
\newcommand{\rd}[0]{\superscript{rd}}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=0.5em,belowskip=0em,abovecaptionskip=0em,belowcaptionskip=0em,
framexbottommargin=-1em,
        basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

% linewrap symbol
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

% thumbnail environment
\newcommand{\thumbheight}{16mm}
\newcommand{\newstrip}{\\[1mm]}
\newenvironment{thumbsequence}{}{\makebox[4mm]{}}

% avoid LaTeX float errors
\usepackage{morefloats}
%\DeclareCaptionType{copyrightbox}

\begin{document}

\title{Getting the Bigger Picture -- Extracting Media Items Covering Events from Multiple Social Networks}

\numberofauthors{3}
\author{
\alignauthor
\textbf{Thomas Steiner}\\
	\affaddr{Univ. Polit\`{e}cnica de Catalunya}\\
	\affaddr{Department LSI}\\
	\affaddr{08034 Barcelona, Spain,}\\
	\affaddr{tsteiner@lsi.upc.edu}
\alignauthor
\textbf{Ruben Verborgh}\\ 	
	\affaddr{Ghent University -- IBBT, ELIS}\\
	\affaddr{Multimedia Lab}\\
	\affaddr{9050 Ghent, Belgium}\\
	\affaddr{ruben.verborgh@ugent.be}	
\and
\alignauthor
\textbf{Raphaël Troncy}\\
	\affaddr{EURECOM}\\
	\affaddr{06560 Sophia Antipolis}\\
	\affaddr{France}\\
	\affaddr{rtroncy@eurecom.fr}
\alignauthor
\textbf{Joaquim Gabarro}\\
	\affaddr{Univ. Polit\`{e}cnica de Catalunya}\\
	\affaddr{Department LSI}\\
	\affaddr{08034 Barcelona, Spain,}\\
	\affaddr{gabarro@lsi.upc.edu}
\alignauthor
\textbf{Rik Van de Walle}\\
	\affaddr{Ghent University -- IBBT, ELIS}\\
	\affaddr{Multimedia Lab}\\
	\affaddr{9050 Ghent, Belgium}\\
	\affaddr{rik.vandewalle@ugent.be}
}

\maketitle

\begin{abstract}
\todo{
Core contributions:
Social network and media platform agnostic media item search, and search results alignment.
Search results semantic enrichment by putting microposts and media items in relation.
Cross-channel popularity analysis.
Visual clustering of images and videos.
}
\end{abstract}

\category{H.3.4}{Information Systems}{Information Storage and Retrieval}[World Wide Web]
\category{H.3.5}{Online Information Services}{Web-based services}

\keywords{\todo{Keywords}}

% Responsible: Raphaël
\section{Introduction} \label{sec:introduction}
\todo{
Say what we do, and what we don't.
Say that we are not competing with the TRECVID guys, as they have all the data, and we have to get it first.
}

\subsection{Used Terms}
In this Subsection, we provide necessary definitions for the terms that we will use throughout this paper.

\paragraph{Social Network}
A \emph{social network} is an online service or \emph{media platform} that focuses on building and reflecting social relations among people who share interests and/or activities.
The boundary between social networks and media platforms is fluid.
Several media platforms~(e.g., YouTube) allow people to upload content,
and optionally allow other people, possibly related to the uploader,
to react to this content in the form of comments, likes, dislikes, and so on.
On other social networks~(e.g., Facebook), people can update their status, post links to stories,
upload content and also give viewers the option to react.
Finally, there are hybrid forms~(e.g., TweetDeck for Twitter using Twitpic) where social networks,
typically via third party applications,
integrate with media platforms.
%RV: I tried to clarify the text a little, but it's still not clear enough to me. I think the first and second types also need a name to distinguish the three forms from each other.

\paragraph{Media Item}
A \emph{media item} in the context of this paper is any video or image file that gets distributed via a social network.
An example can be a baby photo that a fresh parent shares on Facebook. 

\paragraph{Micropost}
A \emph{micropost} is any textual status update that can,
but not has to,
accompany a media item.
An example can be a status update accompanying the previously mentioned baby photo saying ``life is full of wonders, and you are one of them''.
%RV: aha! Could we talk about media items and microposts first, and then describe the different social network types in function of them?
%RV: the quote could make the explanation confusing: are Microposts maybe always things that people just share? I would have a more personal message like "look at our beautiful newborn" or even describing this message instead of quoting it.

\paragraph{Event}
An \emph{event} is an observable occurrence, phenomenon or an extraordinary occurrence with not necessarily defined start and/or end points.
An event with a defined start (May 15, 2011) but without a concrete end
are the 2011 protests in Spain.
An event with both start and end can be a concert.
%RV: I don't know if the Spain protests are known enough. Maybe Occupy Wallstreet?
% Also, this is the first time you give a concrete example.

% Responsible: Thomas
\section{Social Networks \& Media Items}
Media items play a lively role in all social networks,
all of which have a different level of support for media items.
In the following, we introduce a media item support schema for social networks.

%RV: I would decapitalize and possibly add a hyphen (adjective): First-order media item support or even First-order support. Drawback: numerals are gone.
\paragraph{1\st Order Media Item Support}
The social network is centered on media items.
Posting on the social network requires inclusion of a media item.

\paragraph{2\nd Order Media Item Support}
The social network lets users upload media items, however,
posting is also possible without the inclusion of a media item.

\paragraph{3\rd Order Media Item Support}
The social network has no direct support for media items, however,
lets users upload media items on third party media platforms and reference them there.

For this paper, we considered 11 \todo{make sure final number is correct} social networks and in \autoref{tab:networks} provide a categorization according to the schema defined above.
Therefore, the list of considered networks and platforms is necessarily non-exhaustive.
We included those social networks with sufficient market share and popularity, offering mature API capabilities.
For media platforms, the inclusion criterium was guided by a~study by the social media monitoring and analytics firm Sysomos~\cite{Sysomos2011}.

%RV: a space between 2nd and order is missing. However, I did not correct, since it might become a hyphen (see above).
\begin{table*}[htbp]
  \begin{tabular}{ | l | l | l | p{8cm} |}
    \hline
    \textbf{Social Network} & \textbf{URL} & \textbf{Category} & \textbf{Comment}\\
    \hline
	Google+ & \url{http://google.com/+} & 2\nd order & Links to media items are returned via the Google+ API.\\\hline
	MySpace & \url{http://myspace.com} & 2\nd order & Links to media items are returned via the MySpace API.\\\hline
	Facebook & \url{http://facebook.com} & 2\nd order & Links to media items are returned via the Facebook API.\\\hline
	Twitter & \url{http://twitter.com} & 2\nd order & Many people use Twitter in third order mode with other media platforms. In second order mode, links to media items are returned via the Twitter API. In third order mode, Web scraping or media platform API usage are necessary to retrieve links to media items.\\\hline
	Instagram & \url{http://instagram.com} & 1\st order & Links to media items are returned via the Instagram API.\\\hline
	YouTube & \url{http://youtube.com} & 1\st order & Links to media items are returned via the YouTube API.\\\hline
	Flickr & \url{http://flickr.com} & 1\st order & Links to media items are returned via the Flickr API.\\\hline
	MobyPicture & \url{http://mobypicture.com} & 1\st order & Popular as media platform for Twitter. Links to media items are returned via the MobyPicture API.\\\hline
	Twitpic & \url{http://twitpic.com} & 1\st order & Popular as media platform for Twitter. Links to media items must be retrieved via Web scraping.\\\hline
	img.ly & \url{http://img.ly} & 1\st order & Popular as media platform for Twitter. Links to media items must be retrieved via Web scraping.\\\hline
	yfrog & \url{http://yfrog.com} & 1\st order & Popular as media platform for Twitter.   Links to media items must be retrieved via Web scraping.\\
	\hline
  \end{tabular}
  \label{tab:networks}
  \caption{Considered social networks and categorization according to media item support schema.}
\end{table*}

\subsection{Media Item Extraction}
Most social networks offer a search functionality that allows their content to be queried based on search terms,
with or without more advanced search operators like exclusion, inclusion, phrase search, etc.
Each social network has special constraints with regards to supported search operators,
filtering options (e.g., time), or the searchable period.
In the context of this paper, we use the term \emph{media item extraction} to describe the process of leveraging search functionalities of social networks to find references to media items,
which allows for storing those media items in binary form,
i.e., independent from the originating social networks.

\subsection{API Access vs. Web Scraping}
An \emph{Application Programming Interface (API)} in the sense of Web-based API is a programmatic specification intended to be used as an interface by software components on client and server to communicate with each other.

\emph{Web scraping} is the process of automatically extracting information from websites.
Web scraping involves practical solutions based on existing technologies that are often entirely ad hoc.
Examples of such technologies are regular expressions or DOM parsing of Web pages into a DOM tree.
The difference to the somewhat related concept of \emph{screen scraping} is that screen scraping relies on the visual layout of a website, whereas Web scraping relies on the textual and/or hierarchical structure of websites.

Social networks today are very much perceived as ``walled gardens'', excellently illustrated by a cartoon by David Simonds~(\autoref{fig:DavidSimonds}).
%RV: The Orwell reference comes out of the blue (although I like it).
As with Orwell, where some animals are more equal than others, some social networks are more walled than others.
Some social networks~(e.g., Twitter) have full read and write access via specified APIs.
Other social networks~(e.g., Google+) have read access via APIs.
Interestingly, some media platforms~(e.g., Img.ly) have only write support without read support, which requires us to fall back to Web scraping the website in order to retrieve data.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth,trim=16px 17px 12px 15px,clip]{./resources/davidsimonds.jpg}
\caption{David Simonds illustrates social networks as walled gardens due to their (by design) lock-in effects~\cite{DavidSimonds}.}
\label{fig:DavidSimonds}
\end{figure}

% Responsible: Thomas
\section{Implementation Details}
In this Section, we first introduce the common data format used consistently between all considered social networks.
Second, we explain the architecture for different kinds of media item extractors,
and third, show the steps in the media item processing chain.

\subsection{Data Format}
As our approach is agnostic of concrete social networks, we offer a common alignment schema for all considered social networks,
which allows us to treat each social network's data the same way.
The resulting set of metadata for a media item can be seen below:

%RV: description?
\begin{itemize}
  \item	\textbf{Media URL}, the deep link to the media item, e.g., \url{http://farm7.staticflickr.com/6059/6290784192_567346ba6a_o.jpg}.
  \item \textbf{Type}, the type of the media item, one out of ``photo'' or ``video''.
  \item \textbf{Story URL}, the URL of the micropost where the media item appeared, e.g., \url{http://www.flickr.com/photos/96628098@N00/6290784192/}.
  \item \textbf{Message}, the concrete micropost or description text in raw format, e.g., ``Laura. \#lumixg20f17, \#iswc2011, \#internationalsemanticwebconference, \#bonn, \#germany''.
  \item \textbf{Clean}, the concrete cleaned micropost or description text with some characters (e.g., hash sign) removed, e.g., ``Laura. lumixg20f17, iswc2011, internationalsemanticwebconference, bonn, germany''.
  \item \textbf{User}, the URL of the author of the micropost, e.g., \url{http://www.flickr.com/photos/96628098@N00/}.
  \item \textbf{Published}, the timestamp of when the micropost was authored, or the media item was uploaded, e.g., \texttt{2011\-10\-27T12:24:41Z}.
\end{itemize}

\autoref{lst:media} shows the sample output of a media extractor for the social network Google+.

% Use a consistent example from one of the experiments
\begin{lstlisting}[caption={Sample output of the media exctractor showing a Facebook post processed with named entity extraction and disambiguation (slightly edited for legibility).},label={lst:media}]
{
  "Facebook": [
    {
      "mediaurl": "http://video.ak.fbcdn.net/...",
      "storyurl": "https://www.facebook.com/perma-
          link.php?story_fbid=231781590231029&id=
          1254772464",
      "message": {
        "text": "Videoed between Hamburg and Sny-
            der.. Thought I would share.",
        "clean": "Videoed between Hamburg and Sny
            der.. Thought I would share.",
        "entities": [
          [
            {
              "name": "Hamburg",
              "relevance": 0.82274,
              "uri": "http://dbpedia.org/resource/
                  Hamburg"
            },
            {
              "name": "Snyder",
              "relevance": 0.857,
              "uri": "http://dbpedia.org/resource/
                  Snyder,_Texas"
            }
          ]
        ]
      },
      "user": "https://www.facebook.com/
          profile.php?id=1254772464",
      "type": "video",
      "timestamp": 1326371479000,
      "published": "2012-01-12T12:31:19Z"
    }
  ]
}
\end{lstlisting}

\subsection{Media Item Extractors}
In the context of this paper, we have developed media item extractors for the social networks Google+, MySpace, Facebook, Twitter, Instagram, YouTube, and Flickr,
with additional support for the media platforms img.ly, yfrog, MobyPicture, and Twitpic.
As can be seen in \autoref{fig:architecture},
due to the lack of search APIs on some of the involved services,
we were forced to a two-fold approach that involves Web scraping alongside regular API calls.

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{./resources/architecture.pdf}
\caption{The media item extractor architectural overview shows the hybrid approach to the media item extraction process using a combination of API access and Web scraping.}
\label{fig:architecture}
\end{figure*}

\subsection{Media Item Processing}
In this Subsection, we describe our media item processing chain.
As part of the processing chain was manual (see \autoref{sec:deduplication}),
we limited the number of returned results for each media item extractor to 10 items for videos, and 20 items for images.
This explains the tendency to a round number of results in Table X \todo{Reference the results table}.

\subsubsection{Machine Translation}
Social networking is happening at a global scale.
In consequence, many microposts are authored in languages different from English.
In order to still make sense out of those microposts,
rather than limiting ourselves to English-only microposts,
we use machine translation to translate non-English microposts to English.
We use the Google Translate API\footnote{Google Translate API: \url{http://code.google.com/apis/language/translate/v2/getting_started.html}},
which, if the source language parameter is left blank,
first tries to detect the source language,
and subsequently translates the micropost to English.

\subsubsection{Part of Speech Tagging}
Our processing chain supports part of speech tagging via an open source JavaScript library called jspos\footnote{jspos: \url{http://code.google.com/p/jspos/}},
eventually based on Eric Brill's part of speech tagger~\cite{brill1992simple}.
Part of speech tagging at this point does not play an active role yet in the processing chain, however,
in the longterm, we aim for leveraging the additional data for better micropost analysis.

\subsubsection{Named Entity Disambiguation}
Despite their typical relative brevity, microposts still carry a considerable amount of information.
In~\cite{AddingMeaningToMicroposts}, we have shown how meaning can be added to Facebook microposts through named entity recognition and disambiguation.
In this paper, we generalize the approach to common microposts.

\subsubsection{Media Item Deduplication} \label{sec:deduplication}
Some of our long-term goals are to detect the popularity of shared media items across social networks.
This task requires the deduplication of extracted media items.
%RV: CBIR is only defined later on.
For this paper, we have semi-automatically deduplicated the images with the aid of CBIR-based image duplication detection software\footnote{PhotoSweeper: \url{http://itunes.apple.com/us/app/photosweeper/id463362050?mt=12}}, and have completely manually deduplicated the videos.
While in future, the obvious objective is to do this automatically, with this work,
we have created a baseline for planned algorithms tailored to media item deduplication on social networks,
whose special requirements we describe in \autoref{subsec:dataset}.

\paragraph{Image Deduplication}
The image duplication software we employed allows for different algorithms to be used.
We have applied strict pixel-per-pixel comparison for the detection of \emph{exact} duplicates, i.e.,
we do \emph{not} count a resized version of an image as exact duplicate.
Based on bitmap- or histogram-based similarity comparison methods, we introduce the on purpose vague term of \emph{loose} duplicate.
%RV: Maybe not "vague" on purpose but something similar to "wide".
Bitmap similarity is based on comparing pixels of size-reduced bitmaps.
For our comparison, we used bitmaps of the size $128 \times 128$ pixels without smoothed edges.
%RV: Why?
Histogram similarity is based on comparing histograms of size-reduced bitmaps.
This method helps find similar images despite differences in color saturation and lighting.
For both similarity comparison methods, a varying threshold was used.
In our experiments, we could not make out a clear winning setting for all events.
Rather, even for the same event,
only a combination of both similarity comparison methods led to satisfying results,
i.e., a set of images showing loosely the same, as judged by human judgment.
%RV: This seems to conflict with the decisive 128 above.
We would like to highlight, however, that all detected loosely similar images were detected algorithmically,
i.e., the only human intervention was at the algorithm's parameters,
which is an important fact for the objective of fully automating the deduplication process.

\paragraph{Video Deduplication}
We have deduplicated the videos in the dataset by first automatically splitting them in shots~\cite{CrowdsourcingEvent}, and then manually comparing the videos shot-wise.
We considered \emph{exact} duplicates the videos that shared the same shots and same length.
For \emph{loose} duplicates, we manually decided whether the videos showed loosely the same based on human judgment.
We do, at this point, not claim that our results are algorithmically reproducible for loosely similar video detection.

\paragraph{Images Contained in Videos}
While for this paper we did not detect images contained in videos,
the task consists of comparing relevant frames of the video with a target image.
Especially with the focus on event summarization,
the presence of an image in a potentially long video can be considered a signal for the relevance of the corresponding shot of the video.
%RV: I don't entirely understand the purpose of this paragraph.

\section{Experiments}
\label{sec:experiments}
% Responsible: Thomas
For our experiments, we have taken into account several events that happened in the period of January 10 to 19, 2012, and thus were the subject of discussion on various social networks.
We have captured event-related media items and microposts, and made the data available online\footnote{Event-related media items and microposts: \url{http://www.lsi.upc.edu/~tsteiner/experiments/icmr2012/}}. 

\subsection{Considered Events}
%RV: Do we need the footnotes detailing the events?
In this Subsection, we give an objective and short overview on the context of the considered events in order to give the reader the necessary background knowledge.
% Add to each event some categorization: political, technical,…
% Add to each event the duration with start and end if possible.

\paragraph{SOPA Blackout}
The Stop Online Piracy Act (SOPA) is a bill of the United States proposed in 2011 to fight online trafficking in copyrighted intellectual property and counterfeit goods.
On January 18, the English Wikipedia, Reddit, and several other Internet companies coordinated a service blackout to protest SOPA and its sister bill, the Protect IP Act.
Other companies, including Google, posted links and images in an effort to raise awareness\footnote{SOPA Blackout: \url{http://sopablackout.org/learnmore/}}.

\paragraph{Assad Speech}
On January 10, 2012, Syrian President Bashar al-Assad delivered a lengthy televised talk strongly defending his government's actions and motivations, despite world pressure on his embattled government for its 10-month crackdown on protesters\footnote{Assad Speech: \url{http://www.cnn.com/2012/01/10/world/meast/syria-unrest/}}. 

\paragraph{Christian Wulff Case}
Since December 2011, German President Christian Wulff faces controversy over discrepancies in statements about a loan while governor of Lower Saxony.
When the affair settled down, it was revealed that he had applied pressure on Springer Press to delay revelations on the issue until he was back from a visit abroad.
When Wulff found out that a tabloid was going to break the story, he left a message on the voice mail of the editor-in-chief in which he threatened to take legal action\footnote{Christian Wulff Case: \url{http://www.spiegel.de/international/germany/0,1518,804631,00.html}}.

\paragraph{Dixville Notch}
Dixville Notch is an unincorporated village in Dixville township of Coos County, New Hampshire, USA, best known in connection with its longstanding middle-of-the-night vote in the U.S. presidential election.
In a tradition that started in the 1960 election, all the eligible voters in Dixville Notch gather at midnight in the ballroom of The Balsams.
This year, on January 10, 2012, the voters cast their ballots and the polls officially closed one minute later\footnote{Dixville Notch: \url{http://www.washingtonpost.com/2012/01/09/gIQANslKnP_story.html}}.

\paragraph{Free Mobile Launch}
Free Mobile is a French mobile broadband company, part of the Iliad group.
On January 10, 2012, a long-awaited mobile phone package for \EUR{19.99} with calls included to 40 countries, texts, multimedia messages and Internet was announced by the Iliad group's Chief Strategy Officer, Xavier Niel\footnote{Free Mobile Launch: \url{http://www.nytimes.com/2012/01/11/technology/iliad-takes-aim-at-top-mobile-operators-in-france.html}}.

\paragraph{Costa Concordia Disaster}
The Costa Concordia is an Italian cruise ship that hit a reef and partially sank on January 13, 2012 off the Italian coast.
The vessel ran aground at Isola del Giglio, Tuscany, resulting in the evacuation of 4,211 people on board\footnote{Coosta Concordia Disaster: \url{http://www.costacruise.com/B2C/USA/Info/concordia_statement.htm}}.

\paragraph{CES Las Vegas}
The International Consumer Electronics Show (CES) is a major technology-related trade show held each January in the Las Vegas Convention Center.
Not open to the public, the Consumer Electronics Association-sponsored show typically hosts previews of products and new product announcements\footnote{CES Las Vegas: \url{http://www.cesweb.org/aboutcea.asp}}.

\paragraph{Cut the Rope Launch}
% Sub-event of CES Las Vegas
On January 10, 2012 during Microsoft's keynote at CES, the HTML5 version of the popular mobile game \textit{Cut the Rope} was announced\footnote{Cut the Rope Launch: \url{http://ces.cnet.com/8301-33377_1-57356403/}}.

\paragraph{Ubuntu TV Launch}
% Sub-event of CES Las Vegas
Ubuntu TV by Canonical, based on the user interface Unity, is a variant of the Ubuntu operating system, designed to be a Linux distribution specially adapted for embedded systems in televisions. It was announced by Canonical on January 10, 2012, at CES\footnote{Ubuntu TV: \url{http://www.theverge.com/2012/1/9/2695387/ubuntu-tv-video-hands-on}}.

\subsection{Dataset}
\label{subsec:dataset}
During the examination of our dataset, we observed that the process of image deduplication is by no means a solved issue.
Content-based image retrieval (CBIR) uses features like color, texture, and shape to search images from large-scale databases.
The same technique, however, can also be used for the deduplication of photographs~\cite{Pattabhi2011}.
We used a CBIR-based image duplication detection software\footnote{PhotoSweeper: \url{http://itunes.apple.com/us/app/photosweeper/id463362050?mt=12}} that allows for manual algorithm and threshold selection to detect duplicates in the dataset.
For each considered event, we manually selected the best settings to limit the number of duplicate misses and false positives.
The main problem with the dataset is its diversity.
It ranges from entirely sharp screenshots in all sorts of formats (e.g., for the SOPA Blackout), to blurry cell phone images in standard photo formats (e.g., for the Free Mobile Launch).
A common performance tweak to speed up the duplication detection process is to shrink images to quadratic bitmaps.
In the context of our dataset, however, this approach is counterproductive, as a screenshot of a rectangular IAB $728 \times 90$ ``leaderboard'' banner is treated the same as a standard 3.1 megapixels ($2048 \times 1536$) cell phone photo.
%RV: I (it's probably just me) don't understand the problem here: isn't this same treatment what we want?
In practice, this led to many incorrect results, requiring manual deduplication with some events.
Our data set contained 448 images with an average file size of $\sim$0.7MB and 143 videos.

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 3 & 2\\
    MySpace & 0 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 0\\
    MobyPicture & 0 & 0\\
    Twitpic & 0 & 0\\
    \hline
  \end{tabular}
  \label{tab:assad}
  \caption{\textbf{Assad Speech} -- Network Distribution. \textbf{Total images:} 15. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 0\\
    MySpace & 0 & 0\\
    Facebook & 0 & 2\\
    Twitter & 5 & 0\\
    Instagram & 20 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 0\\
    MobyPicture & 1 & 0\\
    Twitpic & 19 & 0\\
    \hline
  \end{tabular}
  \label{tab:sopa}
  \caption{\textbf{Blackout SOPA} -- Network Distribution. \textbf{Total images:} 60. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 3\\
    MySpace & 0 & 0\\
    Facebook & 0 & 1\\
    Twitter & 2 & 0\\
    Instagram & 20 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 6\\
    MobyPicture & 1 & 0\\
    Twitpic & 20 & 0\\
    \hline
  \end{tabular}
  \label{tab:ces}
  \caption{\textbf{CES Las Vegas} -- Network Distribution. \textbf{Total images:} 58. \textbf{Total videos:} 20.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 7 & 0\\
    MySpace & 8 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 2 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 2\\
    MobyPicture & 3 & 0\\
    Twitpic & 20 & 0\\
    \hline
  \end{tabular}
  \label{tab:wulff}
  \caption{\textbf{Christian Wulff Case} -- Network Distribution. \textbf{Total images:} 52. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 15 & 1\\
    MySpace & 10 & 0\\
    Facebook & 0 & 1\\
    Twitter & 3 & 0\\
    Instagram & 20 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 10\\
    MobyPicture & 4 & 0\\
    Twitpic & 18 & 0\\
    \hline
  \end{tabular}
  \label{tab:concordia}
  \caption{\textbf{Costa Concordia Disaster} -- Network Distribution. \textbf{Total images:} 80. \textbf{Total videos:} 22.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 1\\
    MySpace & 6 & 0\\
    Facebook & 0 & 0\\
    Twitter & 4 & 0\\
    Instagram & 20 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 10\\ 
    MobyPicture & 20 & 0\\
    Twitpic & 20 & 0\\
    \hline
  \end{tabular}
  \label{tab:rope}
  \caption{\textbf{Cut the Rope Launch} -- Network Distribution. \textbf{Total images:} 85. \textbf{Total videos:} 21.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 4 & 1\\
    MySpace & 9 & 0\\
    Facebook & 0 & 0\\
    Twitter & 3 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 3\\
    Flickr & 10 & 10\\ 
    MobyPicture & 0 & 0\\
    Twitpic & 1 & 0\\
    \hline
  \end{tabular}
  \label{tab:dixville}
  \caption{\textbf{Dixville Notch} -- Network Distribution. \textbf{Total images:} 27. \textbf{Total videos:} 14.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 6 & 0\\
    MySpace & 1 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 20 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 0\\ 
    MobyPicture & 2 & 0\\
    Twitpic & 20 & 0\\
    \hline
  \end{tabular}
  \label{tab:freemobile}
  \caption{\textbf{Free Mobile Launch} -- Network Distribution. \textbf{Total images:} 61. \textbf{Total videos:} 10.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 6 & 1\\
    MySpace & 0 & 0\\
    Facebook & 0 & 0\\
    Twitter & 0 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 10\\
    Flickr & 10 & 9\\ 
    MobyPicture & 2 & 0\\
    Twitpic & 2 & 0\\
    \hline
  \end{tabular}
  \label{tab:ubuntutv}
  \caption{\textbf{Ubuntu TV Launch} -- Network Distribution. \textbf{Total images:} 20. \textbf{Total videos:} 20.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Event} & \textbf{Exact Duplicate Images} & \textbf{Loose Duplicate Images}\\
    \hline
    Assad Speech & 0 images in 0 sequences & 2 images in 1 sequence\\
    Blackout SOPA & 0 images in 0 sequences & 14 images in 4 sequences\\
    CES Las Vegas & 0 images in 0 sequences & 9 images in 3 sequences\\
    Christian Wulff Case & 4 images in 2 sequences & 0 images in 0 sequences\\
    Costa Concordia & 0 images in 0 sequences & 6 images in 3 sequences\\
    Cut the Rope Launch & 2 images in 1 sequence & 15 images in 5 sequences\\
    Dixville Notch & 2 images in 1 sequence & 2 images in 1 sequence\\
    Free Mobile Launch & 2 images in 1 sequence & 16 images in 7 sequences\\
    Ubuntu TV Launch & 0 images in 0 sequences & 5 images in 1 sequence\\
    \hline
  \end{tabular}
  \label{tab:duplicateimages}
  \caption{Exact and loose duplicate images per event.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Event} & \textbf{Exact Duplicate Videos} & \textbf{Loose Duplicate Videos}\\
    \hline
    Assad Speech\footnote{3 videos were no longer available due to account termination by the users (two cases) and video takedown by the user (one case).}\todo{Footnote not shown. Needs manual fix?} & 0 videos in 0 sequences & 2 videos in 1 sequence\\
    Blackout SOPA & 2 videos in 1 sequence & 0 videos in 0 sequences\\
    CES Las Vegas & 0 videos in 0 sequences & 2 videos in 1 sequence\\
    Christian Wulff Case & 0 videos in 0 sequences & 0 videos in 0 sequences\\
    Costa Concordia & 0 videos in 0 sequences & 0 videos in 0 sequences\\
    Cut the Rope Launch & 0 videos in 0 sequences & 14 videos in 3 sequences\footnote{6 videos in 2 sequences were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Dixville Notch & 2 videos in 1 sequence & 0 videos in 0 sequences\footnote{1 video was no longer available due to video takedown by the user. 11 videos were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Free Mobile Launch & 0 videos in 0 sequences & 0 videos in 0 sequences\footnote{5 videos were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Ubuntu TV Launch & 4 videos in 1 sequence & 9 videos in 4 sequences\footnote{8 videos in 3 sequences were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    \hline
  \end{tabular}
  \label{tab:duplicatevideos}
  \caption{Exact and loose duplicate videos per event.}
\end{table*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate4.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate6.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate7.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate8.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate9.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate10.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate11.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate12.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\setlength\fboxsep{0pt}
		\setlength\fboxrule{0.1mm}
		\fbox{\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate13.png}}
		\fbox{\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate14.jpg}}
	\end{thumbsequence}
	\caption{\textbf{Blackout SOPA:} 14 images in 4 sequences loose duplicate.}
	\label{fig:sopa}
\end{centering}
\end{figure*}


\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/assad/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/assad/looseduplicate2.jpg}
	\end{thumbsequence}
	\caption{\textbf{Assad Speech:} 2 images in 1 sequence loose duplicate.}
	\label{fig:assad}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate4.jpg}
	\end{thumbsequence}
	\caption{\textbf{Christian Wulff Case:} 4 images in 2 sequences exact duplicate.}
	\label{fig:wulff}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/dixville/exactduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/dixville/exactduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/dixville/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/dixville/looseduplicate2.jpg}
	\end{thumbsequence}
	\caption{\textbf{Dixville Notch:} 2 images in 1 sequence exact duplicate, 2 images in 1 sequence loose duplicate.}
	\label{fig:dixville}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/exactduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/free/exactduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate1.png}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate2.png}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate6.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate7.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate8.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate9.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate10.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate11.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate4.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate12.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate13.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate14.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate15.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate16.png}
	\end{thumbsequence}
	\caption{\textbf{Free Mobile Launch:} 2 images in 1 sequence exact duplicate, 16 images in 7 sequences loose duplicate.}
	\label{fig:freemobile}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate4.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate6.jpg}
	\end{thumbsequence}
	\caption{\textbf{Costa Concordia Disaster:} 6 images in 3 sequences loose duplicate.}
	\label{fig:concordia}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate4.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate5.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate6.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate7.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate8.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate9.jpg}
	\end{thumbsequence}
	\caption{\textbf{CES Las Vegas:} 9 images in 3 sequences loose duplicate.}
	\label{fig:ces}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/exactduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/exactduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate3.png}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate4.png}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate6.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate7.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate8.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate9.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate12.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate13.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate14.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate15.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate10.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate11.jpg}
	\end{thumbsequence}
	\caption{\textbf{Cut the Rope Launch:} 2 images in 1 sequence exact duplicate, 15 images in 5 sequences loose duplicate.}
	\label{fig:rope}
\end{centering}
\end{figure*}

\begin{figure*}
\begin{centering}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate4.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate5.png}
	\end{thumbsequence}
	\caption{\textbf{Ubuntu TV Launch:} 5 images in 1 sequence loose duplicate.}
	\label{fig:ubuntutv}
\end{centering}
\end{figure*}

\subsection{Results Interpretion and Discussion}
Looking at our dataset, it becomes evident that the amount of exact duplicate images and videos is inferior compared to the amount of loose duplicates.
On the one hand, this is owed to applied search operators that exclude reshared microposts like, e.g., ReTweets of the same tweet.
So in order for the same media item to appear twice in our results, two users have to author different microposts referencing the same media item\footnote{Using the same, or a different URL.} rather than just one user resharing a micropost from the other.
On the other hand, it is also owed to our strict definition of exact duplicate, which counts resized versions of the same media item as different.
We argue that this, while at the first sight counterintuitive,
makes sense as someone had to actively process the media item.
A good example is the Blackout SOPA event where the text in the first sequence in \autoref{fig:sopa} is identical, however,
the images have different file sizes, resolutions, and paddings.
This implies that someone has taken the initial image, processed it by, e.g.,
cropping it, and only then used the modified version.
A similar example is the second sequence in \autoref{fig:concordia} of the Costa Concordia Disaster, where the chimney got cut off.

By clustering loose duplicate media items in sequences,
we aim at boosting diversity in the returned set of media items for a given event:
if there are many media items for an event,
in the long-term we try to only show the most relevant ones from each sequence.
This will require a media item ranking formula that will include a combination of different categories of ranking criteria:

\paragraph{Visual Features}
Extracted visual features from media items can help judge their quality.
Sharp is better than blurry, higher contrast is better than lower contrast, etc.
An interesting observation can be made with videos: videos with less or just one camera shot are more probable to be produced by amateurs (in tendency more authentic),
whereas videos with more shots are more probable to be produced by professionals (in tendency more official and credible).
Depending on the event, the one might be preferred over the other.

\paragraph{Low-level Features}
Common low-level features like resolution, file size, but also the presence of Exif data for, e.g.,
geolocation, are good quality indicators.
Longer videos are better than shorter videos, higher resolution is better than lower resolution, etc.

\paragraph{Social Features}
Media items belonging to the same sequence can have different popularity,
both globally across social networks, or on specific social networks.
Combining network-specific signals,
like number of ReTweets on Twitter, Likes on Facebook, +1s on Google+, views on YouTube;
and generic signals, like number of comments to a micropost,
a good social popularity indicator can be generated.
This can also include user diversity, i.e.,
featuring content from different users rather than just content from one user.

\paragraph{Textual Features}
Important context is available in form of microposts that accompany media items.
Named entity disambiguation can reveal valuable insights and, in combination with advanced techniques like face recognition,
help make the right media item choices.
If a user sends a micropost containing a media item about a concert, and if via named entity disambiguation the singer can be mapped back to a Linked Data concept,
the media item can be searched for potentially known objects, e.g., the singer's face.
Applying this combination of visual and textual features,
highly relevant media items can be selected.

\subsection{Result Representation}
Given that a media item ranking was available,
the question of result representation raises.
Should more popular media items be displayed bigger, longer, or with a special decoration like a thicker border in comparison to less popular media items?
For videos, the audio part poses a challenge.
In our experiments we have made the interesting finding that intermixing the audio of all videos of an event oftentimes generates a very characteristic ``noise cloud''.
A good example is the Assad Speech event, where a mix of Arabic voices blends nicely with the speech of a US politician.
A different example is the CES Las Vegas event, where the atmosphere of a big exposition with music, announcements, and technical analyses becomes alive.


% Responsible: Ruben
\section{Related Work} \label{sec:relatedwork}
% Separate related work in fields:
% - media item collection from social networks
% - event summarization

% Highlight what Twitter is doing (most popular image/video)
% Google news top stories
% Automatic gallery creation

A~first category of related work includes research that aims to collect, align, and organize media for trends or events.
Liu \emph{et al.} combine semantic inferencing and visual analysis to automatically find media to illustrate events~\cite{Liu2011}.
They interlink large datasets of event metadata and media with the Linking Open Data Cloud~\cite{LODcloud}.
Approaches for alignment use visual, temporal, and spacial similarity measures to map multiple photo streams of the same events~\cite{Yang2011}.
Other ways to collect and order media from social networks use user-driven metadata such as geospatial information~\cite{Crandall}.

Another relevant work area is duplicate and near-duplicate media detection.
As evident from several experiments and mentioned in \autoref{subsec:dataset},
providing unique media content to users is still an unsolved problem.
Work on ordinal measures for image correspondence started in the last decade of the 20\superscript{th}~century~\cite{Bhat}.
Recently, Chum \emph{et al.} have proposed a near-duplicate image detection method using MinHash and tf--idf weighting~\cite{Chum}.
A~method for both images and video has been proposed by Yang \emph{et al.}~\cite{Yang}.
Specialized methods for video exist as well~\cite{Min, Wu}, an excellent survey of which has been conducted by Lian \emph{et al.}~\cite{Lian}.

When unique media items have been collected, the remaining task is to summarize events by selecting the most relevant media fragments.
An article by Fabro and B\"osz\"orm\'enyi~\cite{Fabro2012} details the summarization and presentation of events from content retrieved from social media.
Nowadays, many domain-specific methods already exhibit good accuracy, for example in the sports domain~\cite{Li1,Li2}.
However, the challenge in this field is to find methods that are content-agnostic.
Methods that exploit semantic information~(\emph{e.g.}, \cite{Chen}) will likely provide high-quality results in the future,
but today's most relevant summaries are produced by user interaction~\cite{Olsen}.

% Temporarily added to simplify writing and reading Future Work section
%\clearpage

\section{Conclusion and Future Work}
% Conclusion
\todo{Write conclusion}

% Future work
% Responsible: Ruben 
In this work, the focus has been on extracting visual media and associated textual messages from social networks.
One possibility for future work would therefore be to pursue our efforts in this direction by supporting more social networks and improving our Web scrapers.
This could significantly improve the quantity and diversity of considered media and messages,
given the fact that different kinds of information are shared to different networks~\cite{ConsumersLook}.

A~more innovative direction is to incorporate techniques from multimedia analysis into the process,
as this will create a~multi-modal environment where different factors are used to organize social content.
This can start with context-independent analyses, such as the content deduplication techniques discussed in \autoref{sec:relatedwork},
or visual quality metrics~(sharpness, contrast\ldots) to display original and high-quality media more prominent in results.
Furthermore, this identification of original content can allow users to choose a~balance between popularity~(favor omnipresent content) and originality~(promote rare content).

Context-aware multimedia analysis will likely bring a~new range of parameters into play,
since many media items contain a~message that is complementary to the text.
For example, facial detection~\cite{ViolaJones} and eventually recognition~\cite{Wright}
can signify the presence of specific people in a~media fragment.
As visual recognition systems grow more powerful, more objects will eventually be recognizable by machines~\cite{Serre},
which would allow generating \emph{visual hashtags} that describe the content \emph{inside} of the media item.
Extracted features in all three categories~(\emph{textual} -- from the micropost,
\emph{visual} -- from the media item,
and \emph{social} -- from the social network in the form of ReTweets, Likes, +1s\ldots)
can also
serve as ranking criteria, be it in isolation, or in combination by
introducing a~ranking formula.
As a~result, this would also positively influence the diversity of automated summarizations.

Nonetheless, it remains important to view the media and the associated text as a~whole,
since the text could convey a~sentiment about or an~explanation of the visual data.
Using named entity recognition~\cite{NERD,AddingMeaningToMicroposts}, the important semantic elements in the message text can be identified to build an understanding of its meaning.
The contents of the message could subsequently be used to narrow down the search space for visual factors, enabling cross-fertilization between the textual and visual analysis, which results in effective, context-aware analysis possibilities~\cite{verborgh_mtap_2011}.

\section*{Acknowledgments}
The research activities as described in this paper were funded by Ghent University, the Interdisciplinary Institute for Broadband Technology (IBBT), the Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT), the Fund for Scientific Research Flanders (FWO Flanders), and the European Union.

This work was partially supported by the European Commission under Grant No. 248296 FP7 \mbox{I-SEARCH} project.
Joaquim Gabarr\'o is partially supported by TIN-2007-66523 (FORMALISM), and SGR 2009-2015 (\mbox{ALBCOM}).

% back to normal size Computer Modern for URLs in bibliography
\let\ttdefault\oldttdefault
\let\url\oldurl

\bibliographystyle{abbrv}
\bibliography{icmr2012}

\balancecolumns
\end{document}