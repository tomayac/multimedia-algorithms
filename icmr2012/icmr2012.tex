%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Getting the Bigger Picture -- Extracting Media Items Covering Events from Multiple Social Networks  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{acm_proc_article-sp}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage[activate=compatibility]{microtype}

% autoref command
\usepackage[pdftex,urlcolor=black,colorlinks=true,linkcolor=black,citecolor=black, draft]{hyperref}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
\def\subfloatautorefname{Subfigure}

\usepackage[lofdepth,lotdepth]{subfig}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{eurosym}
\usepackage{fmtcount}
\usepackage{fancybox}

% give emph a normal fontsize
\let\oldemph\emph
\renewcommand{\emph}[1]{\oldemph{\fontsize{9}{9}\selectfont #1}}

% more readable footnote layout
\renewcommand{\footnotesize}{\fontsize{8pt}{10pt}}
\setlength{\footnotesep}{.5cm}

% todo macro
\usepackage{color}
\newcommand{\todo}[1]{\noindent\textcolor{red}{{\bf \{TODO}: #1{\bf \}}}}
\newenvironment{Todo}{\color{red}{\par{\bf TODO}}\everypar={\color{red}}{}}{}

% superscript for 1st, 2nd, etc.
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newcommand{\subscript}[1]{\ensuremath{_{\textrm{#1}}}}
\newcommand{\st}[0]{\superscript{st}}
\newcommand{\nd}[0]{\superscript{nd}}
\newcommand{\rd}[0]{\superscript{rd}}

% listings and Verbatim environment
\usepackage{fancyvrb}
\usepackage{relsize}
\usepackage{listings}
\usepackage{verbatim}
\newcommand{\defaultlistingsize}{\fontsize{8pt}{9.5pt}}
\newcommand{\inlinelistingsize}{\fontsize{8pt}{11pt}}
\newcommand{\smalllistingsize}{\fontsize{7.5pt}{9.5pt}}
\newcommand{\listingsize}{\defaultlistingsize}
\RecustomVerbatimCommand{\Verb}{Verb}{fontsize=\inlinelistingsize}
\RecustomVerbatimEnvironment{Verbatim}{Verbatim}{fontsize=\defaultlistingsize}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,
        aboveskip=0.5em,belowskip=0em,abovecaptionskip=0em,belowcaptionskip=0em,
framexbottommargin=-1em,
        basicstyle=\ttfamily\listingsize\selectfont}

% use Courier from this point onward
\let\oldttdefault\ttdefault
\renewcommand{\ttdefault}{pcr}
\let\oldurl\url
\renewcommand{\url}[1]{\inlinelistingsize\oldurl{#1}}

% linewrap symbol
\definecolor{grey}{RGB}{130,130,130}
\newcommand{\linewrap}{\raisebox{-.6ex}{\textcolor{grey}{$\hookleftarrow$}}}

% thumbnail environment
\newcommand{\eventtitle}[1]{{\hbox{\strut \textbf{#1}}}}
\newcommand{\thumbheight}{14mm}
\newcommand{\newstrip}{\newline\vspace{-1em}\newline}
\newenvironment{thumbsequence}{}{\makebox[4mm]{}}

% avoid LaTeX float errors
\usepackage{morefloats}
\DeclareCaptionType{copyrightbox}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Beginning of document  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Getting the Bigger Picture -- Extracting Media Items Covering Events from Multiple Social Networks}

\numberofauthors{5}
\author{
\alignauthor
\textbf{Thomas Steiner}\\
	\affaddr{Univ. Polit\`{e}cnica de Catalunya}\\
	\affaddr{Department LSI}\\
	\affaddr{08034 Barcelona, Spain,}\\
	\affaddr{tsteiner@lsi.upc.edu}
\alignauthor
\textbf{Ruben Verborgh}\\ 	
	\affaddr{Ghent University -- IBBT, ELIS}\\
	\affaddr{Multimedia Lab}\\
	\affaddr{9050 Ghent, Belgium}\\
	\affaddr{ruben.verborgh@ugent.be}	
\alignauthor
\textbf{Raphaël Troncy}\\
	\affaddr{EURECOM}\\
	\affaddr{06560 Sophia Antipolis}\\
	\affaddr{France}\\
	\affaddr{rtroncy@eurecom.fr}
\and
\alignauthor
\textbf{Joaquim Gabarro}\\
	\affaddr{Univ. Polit\`{e}cnica de Catalunya}\\
	\affaddr{Department LSI}\\
	\affaddr{08034 Barcelona, Spain,}\\
	\affaddr{gabarro@lsi.upc.edu}
\alignauthor
\textbf{Rik Van de Walle}\\
	\affaddr{Ghent University -- IBBT, ELIS}\\
	\affaddr{Multimedia Lab}\\
	\affaddr{9050 Ghent, Belgium}\\
	\affaddr{rik.vandewalle@ugent.be}
}

\maketitle

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}
Social networks play an increasingly important role to provide authentic and as-it-happens coverage of events.
Representative examples are the 2011 Ut{\o}ya shootings in Norway, or the miracle on the Hudson river in 2009,
where the US Airways flight 1549 ended with an emergency ditching. One of the problem is that media and testimonial
captured by witnesses of those events are spread among multiple social networks. In this paper, we propose a social network 
agnostic approach for the extraction of images and videos covering events, which can be later on processed for the 
automatic generation of visual summaries in the form of media galleries. Our approach includes the alignment of the 
varying search results formats from these social networks while putting the media items in correspondence with the 
status updates and stories they are related to. More precisely, we leverage (i) visual features from media items, 
(ii) textual features from status updates, and (iii) social features from social networks to interpret, de-duplicate, 
cluster and rank media items in order to generate automatically media galleries. We address the technical details of media 
item extraction and media item processing, discuss criteria for media item ranking and envision several visualization options 
for media galleries. Our approach can be tested online at \url{http://tomayac.com/social/}.
\end{abstract}

\category{H.3.4}{Information Systems}{Information Storage and Retrieval}[World Wide Web]
\category{H.3.5}{Online Information Services}{Web-based services}

\keywords{Media Collector, Social Networks, Events, Media Gallery}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} \label{sec:introduction}
Before the rise of social networks, event coverage was mostly an affair of professional news agencies.
With the widespread availability of mobile phones with decent cameras,
event coverage more and more also happens on social networks,
with sometimes news agencies featuring user-generated content from otherwise unknown social network users.
Some examples with global impact were the shootings in Ut{\o}ya (first appearance on Twitter), the capture and arrest of Muammar Gaddafi (first appearance on YouTube), and the emergency ditching of a plane in Hudson river (first appearance on Twitpic).
However, also for events without global impact like local concerts,
interesting media items are available,
albeit distributed over potentially many social networks.
In response to this challenge, in this paper, we propose a social network agnostic approach for the extraction of images and videos covering events.
We want to emphasize that we do \emph{not} perform event detection, i.e.,
we are aware of the occurrence of events and use specific human-chosen search terms to find media about them.
Different from \mbox{ImageCLEF} and \mbox{TRECVID}, that both deal with content-based analysis of and retrieval from images and videos,
our approach deals with the separate additional task of extracting media from social networks in the first place.
In the following paragraphs, we provide necessary definitions for the terms that we will use throughout this paper.

\paragraph{Social Network}
A social network is an online service or media platform that focuses on building and reflecting social relations among people who share interests and/or activities.
The boundary between social networks and media platforms is fluid.
Several media platforms~(e.g., YouTube) allow people to upload content,
and optionally allow other people, possibly related to the uploader,
to react to this content in the form of comments, likes, dislikes, and so on.
On other social networks~(e.g., Facebook), people can update their status, post links to stories,
upload content and also give viewers the option to react.
Finally, there are hybrid forms~(e.g., TweetDeck for Twitter using Twitpic) where social networks,
typically via third party applications,
integrate with media platforms.
%RV: I tried to clarify the text a little, but it's still not clear enough to me. I think the first and second types also need a name to distinguish the three forms from each other.
%TS: The distinction follows later, where I introduce the 1st-3rd order networks. Helps? Has to come earlier? Raphaël?

\paragraph{Media Item}
A media item in the context of this paper is defined as any video or image file that gets distributed via a social network.
An example can be a baby photo that a fresh parent shares on Facebook.

\paragraph{Micropost}
We define the term micropost as any textual status update that can,
but not has to,
accompany a media item.
An example can be a status update accompanying the previously mentioned baby photo. %saying ``life is full of wonders, and you are one of them''.
%RV: aha! Could we talk about media items and microposts first, and then describe the different social network types in function of them?
%TS: Raphaël will take care of this when he restructures (messes up) Sect. 1 and 2.
\paragraph{Event}
An event is here defined as an observable occurrence, phenomenon or an extraordinary occurrence with not necessarily defined start and/or end points.
An event with a defined start point (September 17, 2011), but without a concrete end point, is the Occupy Wall Street movement.
An event with both start and end point can be a concert.

The remainder of this paper is structured as follows: \todo{Outline paper structure}.

% Responsible: Thomas
\section{Social Networks \& Media Items}
Media items play a lively role in all social networks,
all of which have a different level of support for media items.
In the following, we introduce a media item support schema for social networks.

%RV: I would decapitalize and possibly add a hyphen (adjective): First-order media item support or even First-order support. Drawback: numerals are gone.
%TS: Raphaël, your opinion?
\paragraph{1{\st} order media item support}
The social network is centered on media items.
Posting on the social network requires inclusion of a media item.
An example is YouTube.

\paragraph{2{\nd} order media item support}
The social network lets users upload media items, however,
posting is also possible without the inclusion of a media item.
An example is Facebook.

\paragraph{3{\rd} order media item support}
The social network has no direct support for media items, however,
lets users upload media items on third party media platforms and reference them there.
An example was Twitter before the introduction of native photo support.

For this paper, we considered 11 \todo{make sure final number is correct} social networks and in \autoref{tab:networks} \todo{Why does this reference not work???} provide a categorization according to the schema defined above.
Therefore, the list of considered networks and platforms is necessarily non-exhaustive.
We included those social networks with sufficient market share and popularity, offering mature API capabilities.
For media platforms, the inclusion criterium was guided by a~study by the social media monitoring and analytics firm Sysomos~\cite{Sysomos2011}.

%RV: a space between 2nd and order is missing. However, I did not correct, since it might become a hyphen (see above).
%TS: corrected the spacing. Not sure about the hyphen. Raphaël? What do you prefer?
\begin{table*}[htbp]
  \begin{tabular}{ | l | l | l | p{8cm} |}
    \hline
    \textbf{Social Network} & \textbf{URL} & \textbf{Category} & \textbf{Comment}\\
    \hline
	Google+ & \url{http://google.com/+} & 2{\nd} order & Links to media items are returned via the Google+ API.\\\hline
	MySpace & \url{http://myspace.com} & 2{\nd} order & Links to media items are returned via the MySpace API.\\\hline
	Facebook & \url{http://facebook.com} & 2{\nd} order & Links to media items are returned via the Facebook API.\\\hline
	Twitter & \url{http://twitter.com} & 2{\nd} / 3{\rd} order & Many people use Twitter in third order mode with other media platforms. In second order mode, links to media items are returned via the Twitter API. In third order mode, Web scraping or media platform API usage are necessary to retrieve links to media items.\\\hline
	Instagram & \url{http://instagram.com} & 1{\st} order & Links to media items are returned via the Instagram API.\\\hline
	YouTube & \url{http://youtube.com} & 1{\st} order & Links to media items are returned via the YouTube API.\\\hline
	Flickr & \url{http://flickr.com} & 1{\st} order & Links to media items are returned via the Flickr API.\\\hline
	MobyPicture & \url{http://mobypicture.com} & 1{\st} order & Popular as media platform for Twitter. Links to media items are returned via the MobyPicture API.\\\hline
	Twitpic & \url{http://twitpic.com} & 1{\st} order & Popular as media platform for Twitter. Links to media items must be retrieved via Web scraping.\\\hline
	img.ly & \url{http://img.ly} & 1{\st} order & Popular as media platform for Twitter. Links to media items must be retrieved via Web scraping.\\\hline
	yfrog & \url{http://yfrog.com} & 1{\st} order & Popular as media platform for Twitter.   Links to media items must be retrieved via Web scraping.\\
	\hline
  \end{tabular}
  \label{tab:networks}
  \caption{Considered social networks and categorization according to the media item support schema defined in Subsection X \todo{Make sure to add the reference wherever the media item support schema might end up}.}
\end{table*}

\subsection{Media Item Extraction}
Most social networks offer a search functionality that allows their content to be queried based on search terms,
with or without more advanced search operators like exclusion, inclusion, phrase search, etc.
Each social network has special constraints with regards to supported search operators,
filtering options (e.g., time), or the searchable period.
In the context of this paper, we use the term \emph{media item extraction} to describe the process of leveraging search functionalities of social networks to find references to media items,
which allows for storing those media items in binary form,
i.e., independent from the originating social networks.

\subsection{API Access vs. Web Scraping}
An \emph{Application Programming Interface (API)} in the sense of Web-based API is a programmatic specification intended to be used as an interface by software components on client and server to communicate with each other.

\emph{Web scraping} is the process of automatically extracting information from websites.
Web scraping involves practical solutions based on existing technologies that are often entirely ad hoc.
Examples of such technologies are regular expressions or DOM parsing of Web pages into a DOM tree.
The difference to the somewhat related concept of \emph{screen scraping} is that screen scraping relies on the visual layout of a website, whereas Web scraping relies on the textual and/or hierarchical structure of websites.

Social networks today are very much perceived as ``walled gardens'', excellently illustrated by a cartoon by David Simonds~(\autoref{fig:DavidSimonds}).
%RV: The Orwell reference comes out of the blue (although I like it).
%TS: I found it fun, but if it adds confusion, we remove it. Raphaël?
As with Orwell, where some animals are more equal than others, some social networks are more walled than others.
Several social networks~(e.g., Twitter) have full read and write access via specified APIs.
Others~(e.g., Google+) have read-only access via APIs.
Interestingly, some media platforms~(e.g., img.ly) have write-only without read support, which requires us to fall back to Web scraping the website in order to retrieve data.

\begin{figure}
\centering
\includegraphics[width=1.0\linewidth,trim=16px 17px 12px 15px,clip]{./resources/davidsimonds.jpg}
\caption{David Simonds illustrates social networks as walled gardens due to their (by design) lock-in effects~\cite{DavidSimonds}.}
\label{fig:DavidSimonds}
\end{figure}

% Responsible: Thomas
\section{Implementation Details}
In this Section, we first introduce the common data format used consistently between all considered social networks.
Second, we explain the architecture for different kinds of media item extractors,
and third, show the steps in the media item processing chain.

\subsection{Data Format} \label{sec:dataformat}
As our approach is agnostic of concrete social networks, we offer a common alignment schema for all considered social networks,
which allows us to treat each social network's data the same way.
The resulting set of metadata for a media item can be seen below:

\begin{description}
  \item[Media URL] The deep link to the media item, e.g., \url{http://farm7.staticflickr.com/6059/6290784192_567346ba6a_o.jpg}.
  \item[Type] The type of the media item, one out of ``photo'' or ``video''.
  \item[Story URL] The URL of the micropost where the media item appeared, e.g., \url{http://www.flickr.com/photos/96628098@N00/6290784192/}.
  \item[Message] The concrete micropost or description text in raw format, e.g., ``Laura. \#lumixg20f17, \#iswc2011, \#internationalsemanticwebconference, \#bonn, \#germany''.
  \item[Clean] The concrete cleaned micropost or description text with some characters (e.g., hash sign) removed, e.g., ``Laura. lumixg20f17, iswc2011, internationalsemanticwebconference, bonn, germany''.
  \item[User] The URL of the author of the micropost, e.g., \url{http://www.flickr.com/photos/96628098@N00/}.
  \item[Published] The timestamp of when the micropost was authored, or the media item was uploaded, e.g., \texttt{2011\-10\-27T12:24:41Z}.
\end{description}

\autoref{lst:media} shows the sample output of a media extractor for the social network Facebook.

% Use a consistent example from one of the experiments
\begin{lstlisting}[caption={Sample output of the media extractor showing a Facebook post processed with named entity extraction and disambiguation (slightly edited for legibility).},label={lst:media}]
{
  "Facebook": [
    {
      "mediaurl": "http://video.ak.fbcdn.net/...",
      "storyurl": "https://www.facebook.com/perma-
          link.php?story_fbid=231781590231029&id=
          1254772464",
      "message": {
        "text": "Videoed between Hamburg and Sny-
            der.. Thought I would share.",
        "clean": "Videoed between Hamburg and Sny-
            der.. Thought I would share.",
        "entities": [
          [
            {
              "name": "Hamburg",
              "relevance": 0.82274,
              "uri": "http://dbpedia.org/resource/
                  Hamburg"
            },
            {
              "name": "Snyder",
              "relevance": 0.857,
              "uri": "http://dbpedia.org/resource/
                  Snyder,_Texas"
            }
          ]
        ]
      },
      "user": "https://www.facebook.com/
          profile.php?id=1254772464",
      "type": "video",
      "timestamp": 1326371479000,
      "published": "2012-01-12T12:31:19Z"
    }
  ]
}
\end{lstlisting}

\subsection{Media Item Extractors}
In the context of this paper, we have developed media item extractors for the social networks Google+, MySpace, Facebook, Twitter, Instagram, YouTube, and Flickr,
with additional support for the media platforms img.ly, yfrog, MobyPicture, and Twitpic.
We start with a human-chosen search term that is relevant to a known event, e.g., ``costa concordia'' for the Costa Concorida Disaster (see \autoref{sec:concordia}).
This search term gets forwarded to the search APIs of the considered social networks in parallel.
Each social network has a 30 seconds timeout window to deliver its results.
When the timeout is reached, or when all social networks have responded,
the available results are aligned according to the data format defined in \autoref{sec:dataformat}.
Where possible, contained media items are retrieved directly,
or else, obtained via Web scraping.
For some social networks, e.g., MySpace, a combination of Web scraping and API access is required,
as the API does not return all necessary fields as defined in our data format.
In the concrete case of MySpace, albeit the API returns the media items,
the timestamps of when they were uploaded has to be scraped.
The complete data flow can be seen in the architectural diagram in \autoref{fig:architecture}.

\begin{figure*}
\centering
\includegraphics[width=1.0\linewidth]{./resources/architecture.pdf}
\caption{The media item extractor architectural overview shows the hybrid approach to the media item extraction process using a combination of API access and Web scraping.}
\label{fig:architecture}
\end{figure*}

\subsection{Media Item Processing}
In this Subsection, we describe our media item processing chain.
As part of the processing chain was manual (see \autoref{sec:deduplication}),
we have limited the number of returned results for each media item extractor to 10 items for videos, and 20 items for images.
This explains the tendency to round numbers of results in Table X \todo{Reference the results table}, marked with $n+$.

\subsubsection{Machine Translation}
Social networking is happening at a global scale.
In consequence, many microposts are authored in languages different from English.
In order to still make sense out of those microposts,
rather than limiting ourselves to English-only microposts,
we apply machine translation to translate non-English microposts to English.
We use the Google Translate API\footnote{Google Translate API: \url{http://code.google.com/apis/language/translate/v2/getting_started.html}},
which, if the source language parameter is left blank,
first tries to detect the source language,
and subsequently translates the micropost to English.

\subsubsection{Part of Speech Tagging}
Our processing chain supports part of speech tagging via an open source JavaScript library called jspos\footnote{jspos: \url{http://code.google.com/p/jspos/}},
eventually based on Eric Brill's part of speech tagger~\cite{brill1992simple}.
Part of speech tagging at this point does not play an active role yet in the processing chain, however,
in the longterm, we aim for leveraging the additional data for better micropost analysis.

\subsubsection{Named Entity Disambiguation}
Despite their typical relative brevity, microposts still carry a considerable amount of information.ficken
In~\cite{AddingMeaningToMicroposts}, we have shown how meaning can be added to Facebook microposts through named entity recognition and disambiguation.
In this paper, we generalize the approach to common microposts,
based on the learnings from~\cite{NERD}.

\subsubsection{Media Item Deduplication} \label{sec:deduplication}
In order for obtaining interesting media galleries,
the detection of the  popularity of shared media items across social networks is necessary.
This task envolves the deduplication of extracted media items.
For this paper, we have semi-automatically deduplicated the images with the aid of a commercial image duplication detection software\footnote{PhotoSweeper: \url{http://itunes.apple.com/us/app/photosweeper/id463362050?mt=12}}, and have manually deduplicated the videos.
In future, the obvious objective is to do this automatically, however,
with this work, we have created a baseline for specific future algorithms tailored to media item deduplication on social networks,
whose special requirements we describe in \autoref{subsec:dataset}.

\paragraph{Image Deduplication}
The image duplication software we employed allows for different algorithms to be used.
We have applied strict pixel-per-pixel comparison for the detection of \emph{exact} duplicates, i.e.,
we do \emph{not} count a resized version of an image as exact duplicate.
Based on bitmap- or histogram-based similarity comparison methods, we introduce the relatively wide term of \emph{loose} duplicate.
Bitmap similarity is based on comparing pixels of size-reduced bitmaps.
For our comparison, we used bitmaps of the size $128 \times 128$ pixels without smoothed edges, which corresponds to the best quality settings in the software.
Histogram similarity is based on comparing histograms of size-reduced bitmaps.
This method helps find similar images despite differences in color saturation and lighting.
For both similarity comparison methods, a varying threshold was used.
In our experiments, we could not make out a clear winning setting for all events.
Rather, even for the same event,
only a combination of both similarity comparison methods led to satisfactory results,
i.e., to a set of loosely duplicate images that also a human being would have chosen.
%RV: This seems to conflict with the decisive 128 above.
%TS: Both methods use the same bitmap sizes. What do you mean?
%RV: You say that there were no winning settings, but apparently 128 is? But it's clear now thanks to the clarification.
We would like to highlight, however, that all detected loosely duplicate images were detected algorithmically,
i.e., the only human intervention was when choosing the algorithm's parameters,
which is an important fact for the objective of fully automating the deduplication process.

\paragraph{Video Deduplication}
We have deduplicated the videos in the dataset by first automatically splitting them in shots~\cite{CrowdsourcingEvent}, and then manually comparing the videos shot-wise.
We considered \emph{exact} duplicates the videos that shared the same shots and same length.
For \emph{loose} duplicates, we manually decided whether the videos showed loosely the same based on human judgment.
We do, at this point, not claim that our results are algorithmically reproducible for loosely similar video detection.

\begin{comment}
\paragraph{Images Contained in Videos}
While for this paper we did not detect images contained in videos,
the task consists of comparing relevant frames of the video with a target image.
Especially with the focus on event summarization,
the presence of an image in a potentially long video can be considered a signal for the relevance of the corresponding shot of the video.
\end{comment}

\section{Experiments}
\label{sec:experiments}
% Responsible: Thomas
For our experiments, we have taken into account several events that happened in the period of January 10 to 19, 2012, and thus were the subject of discussion on various social networks.
We have captured event-related media items and microposts, and made the data available online\footnote{Event-related media items and microposts: \url{http://www.lsi.upc.edu/~tsteiner/experiments/icmr2012/}}.
We invite the reader to browse the data and compile a personal set of loose and exact duplicate media items.
YouTube video URLs are signed with a by now expired time to live,
they can be accessed by following the \texttt{storyurl} link.

\subsection{Considered Events}
In this Subsection, we give a short overview on the context of the considered events in order to give the reader the necessary background knowledge.
% Add to each event some categorization: political, technical,…
% Add to each event the duration with start and end if possible.

\paragraph{Blackout SOPA}
The Stop Online Piracy Act (SOPA) is a bill of the United States proposed in 2011 to fight online trafficking in copyrighted intellectual property and counterfeit goods.
On January 18, the English Wikipedia, Reddit, and several other Internet companies coordinated a service blackout to protest SOPA and its sister bill, the Protect IP Act.
Other companies, including Google, posted links and images in an effort to raise awareness.
% \footnote{Blackout SOPA: \url{http://sopablackout.org/learnmore/}}

\paragraph{Assad Speech}
On January 10, 2012, Syrian President Bashar al-Assad delivered a lengthy televised talk strongly defending his government's actions and motivations, despite world pressure on his embattled government for its 10-month crackdown on protesters.
% \footnote{Assad Speech: \url{http://www.cnn.com/2012/01/10/world/meast/syria-unrest/}}

\paragraph{Christian Wulff Case}
Since December 2011, German President Christian Wulff faces controversy over discrepancies in statements about a loan while governor of Lower Saxony.
When the affair settled down, it was revealed that he had applied pressure on Springer Press to delay revelations on the issue until he was back from a visit abroad.
When Wulff found out that a tabloid was going to break the story, he left a message on the voice mail of the editor-in-chief in which he threatened to take legal action.
% \footnote{Christian Wulff Case: \url{http://www.spiegel.de/international/germany/0,1518,804631,00.html}}

\paragraph{Dixville Notch}
Dixville Notch is an unincorporated village in Dixville township of Coos County, New Hampshire, USA, best known in connection with its longstanding middle-of-the-night vote in the U.S. presidential election.
In a tradition that started in the 1960 election, all the eligible voters in Dixville Notch gather at midnight in the ballroom of The Balsams.
This year, on January 10, 2012, the voters cast their ballots and the polls officially closed one minute later.
% \footnote{Dixville Notch: \url{http://www.washingtonpost.com/2012/01/09/gIQANslKnP_story.html}}

\paragraph{Free Mobile Launch}
Free Mobile is a French mobile broadband company, part of the Iliad group.
On January 10, 2012, a long-awaited mobile phone package for \EUR{19.99} with calls included to 40 countries, texts, multimedia messages and Internet was announced by the Iliad group's Chief Strategy Officer, Xavier Niel.
% \footnote{Free Mobile Launch: \url{http://www.nytimes.com/2012/01/11/technology/iliad-takes-aim-at-top-mobile-operators-in-france.html}}

\paragraph{Costa Concordia Disaster} \label{sec:concordia}
The Costa Concordia is an Italian cruise ship that hit a reef and partially sank on January 13, 2012 off the Italian coast.
The vessel ran aground at Isola del Giglio, Tuscany, resulting in the evacuation of 4,211 people on board.
% \footnote{Coosta Concordia Disaster: \url{http://www.costacruise.com/B2C/USA/Info/concordia_statement.htm}}

\paragraph{CES Las Vegas}
The International Consumer Electronics Show (CES) is a major technology-related trade show held each January in the Las Vegas Convention Center.
Not open to the public, the Consumer Electronics Association-sponsored show typically hosts previews of products and new product announcements.
% \footnote{CES Las Vegas: \url{http://www.cesweb.org/aboutcea.asp}}

\paragraph{Cut the Rope Launch}
% Sub-event of CES Las Vegas
On January 10, 2012 during Microsoft's keynote at CES, the HTML5 version of the popular mobile game \textit{Cut the Rope} was announced.
% \footnote{Cut the Rope Launch: \url{http://ces.cnet.com/8301-33377_1-57356403/}}

\paragraph{Ubuntu TV Launch}
% Sub-event of CES Las Vegas
Ubuntu TV by Canonical, based on the user interface Unity, is a variant of the Ubuntu operating system, designed to be a Linux distribution specially adapted for embedded systems in televisions. It was announced by Canonical on January 10, 2012, at CES.
% \footnote{Ubuntu TV: \url{http://www.theverge.com/2012/1/9/2695387/ubuntu-tv-video-hands-on}}

\subsection{Dataset} \label{subsec:dataset}
Our data set contained 448 images with an average file size of $\sim$0.7MB and 143 videos.
During the examination of our dataset, we observed that the process of image deduplication is by no means a solved issue.
Content-based image retrieval (CBIR) uses features like color, texture, and shape to search images from large-scale databases.
The same technique, however, can also be used for the deduplication of photographs~\cite{Pattabhi2011}.
We used a CBIR-based image duplication detection software\footnote{PhotoSweeper: \url{http://itunes.apple.com/us/app/photosweeper/id463362050?mt=12}} that allows for manual algorithm and threshold selection to detect duplicates in the dataset.
For each considered event, we manually selected the best settings to limit the number of duplicate misses and false positives.
The main problem with the dataset is its diversity.
It ranges from entirely sharp screenshots in all sorts of formats (e.g., screenshots of the Google homepage for the Blackout SOPA event), to blurry cell phone images in standard photo formats (e.g., photos of the stage for the Free Mobile Launch event).
A common performance tweak to speed up the duplication detection process is to shrink images to quadratic bitmaps.
In the context of our dataset, however, this approach is counterproductive, as a screenshot of a rectangular IAB $728 \times 90$ ``leaderboard'' banner is treated the same as a standard 3.1 megapixels ($2048 \times 1536$) cell phone photo.
In practice, shrinking a wide rectangular banner to a square led to many incorrect results, requiring manual deduplication with the Blackout SOPA event.

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 3 & 2\\
    MySpace & 0 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 0\\
    MobyPicture & 0 & 0\\
    Twitpic & 0 & 0\\
    \hline
  \end{tabular}
  \label{tab:assad}
  \caption{\textbf{Assad Speech} -- Network Distribution. \textbf{Total images:} 15. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 0\\
    MySpace & 0 & 0\\
    Facebook & 0 & 2\\
    Twitter & 5 & 0\\
    Instagram & 20+ & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 0\\
    MobyPicture & 1 & 0\\
    Twitpic & 19 & 0\\
    \hline
  \end{tabular}
  \label{tab:sopa}
  \caption{\textbf{Blackout SOPA} -- Network Distribution. \textbf{Total images:} 60. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 3\\
    MySpace & 0 & 0\\
    Facebook & 0 & 1\\
    Twitter & 2 & 0\\
    Instagram & 20+ & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 6\\
    MobyPicture & 1 & 0\\
    Twitpic & 20+ & 0\\
    \hline
  \end{tabular}
  \label{tab:ces}
  \caption{\textbf{CES Las Vegas} -- Network Distribution. \textbf{Total images:} 58. \textbf{Total videos:} 20.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 7 & 0\\
    MySpace & 8 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 2 & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 2\\
    MobyPicture & 3 & 0\\
    Twitpic & 20+ & 0\\
    \hline
  \end{tabular}
  \label{tab:wulff}
  \caption{\textbf{Christian Wulff Case} -- Network Distribution. \textbf{Total images:} 52. \textbf{Total videos:} 12.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 15 & 1\\
    MySpace & 10+ & 0\\
    Facebook & 0 & 1\\
    Twitter & 3 & 0\\
    Instagram & 20+ & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 10+\\
    MobyPicture & 4 & 0\\
    Twitpic & 18 & 0\\
    \hline
  \end{tabular}
  \label{tab:concordia}
  \caption{\textbf{Costa Concordia Disaster} -- Network Distribution. \textbf{Total images:} 80. \textbf{Total videos:} 22.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 5 & 1\\
    MySpace & 6 & 0\\
    Facebook & 0 & 0\\
    Twitter & 4 & 0\\
    Instagram & 20+ & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10 & 10+\\
    MobyPicture & 20+ & 0\\
    Twitpic & 20+ & 0\\
    \hline
  \end{tabular}
  \label{tab:rope}
  \caption{\textbf{Cut the Rope Launch} -- Network Distribution. \textbf{Total images:} 85. \textbf{Total videos:} 21.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 4 & 1\\
    MySpace & 9 & 0\\
    Facebook & 0 & 0\\
    Twitter & 3 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 3\\
    Flickr & 10 & 10+\\
    MobyPicture & 0 & 0\\
    Twitpic & 1 & 0\\
    \hline
  \end{tabular}
  \label{tab:dixville}
  \caption{\textbf{Dixville Notch} -- Network Distribution. \textbf{Total images:} 27. \textbf{Total videos:} 14.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 6 & 0\\
    MySpace & 1 & 0\\
    Facebook & 0 & 0\\
    Twitter & 2 & 0\\
    Instagram & 20+ & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 0\\
    MobyPicture & 2 & 0\\
    Twitpic & 20+ & 0\\
    \hline
  \end{tabular}
  \label{tab:freemobile}
  \caption{\textbf{Free Mobile Launch} -- Network Distribution. \textbf{Total images:} 61. \textbf{Total videos:} 10.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Social Network} & \textbf{Images} & \textbf{Videos}\\
    \hline
    Google+ & 6 & 1\\
    MySpace & 0 & 0\\
    Facebook & 0 & 0\\
    Twitter & 0 & 0\\
    Instagram & 0 & 0\\
    YouTube & 0 & 10+\\
    Flickr & 10+ & 9\\
    MobyPicture & 2 & 0\\
    Twitpic & 2 & 0\\
    \hline
  \end{tabular}
  \label{tab:ubuntutv}
  \caption{\textbf{Ubuntu TV Launch} -- Network Distribution. \textbf{Total images:} 20. \textbf{Total videos:} 20.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Event} & \textbf{Exact Duplicate Images} & \textbf{Loose Duplicate Images}\\
    \hline
    Assad Speech & 0 images in 0 sequences & 2 images in 1 sequence\\
    Blackout SOPA & 0 images in 0 sequences & 14 images in 4 sequences\\
    CES Las Vegas & 0 images in 0 sequences & 9 images in 3 sequences\\
    Christian Wulff Case & 4 images in 2 sequences & 0 images in 0 sequences\\
    Costa Concordia & 0 images in 0 sequences & 6 images in 3 sequences\\
    Cut the Rope Launch & 2 images in 1 sequence & 15 images in 5 sequences\\
    Dixville Notch & 2 images in 1 sequence & 2 images in 1 sequence\\
    Free Mobile Launch & 2 images in 1 sequence & 16 images in 7 sequences\\
    Ubuntu TV Launch & 0 images in 0 sequences & 5 images in 1 sequence\\
    \hline
  \end{tabular}
  \label{tab:duplicateimages}
  \caption{Exact and loose duplicate images per event.}
\end{table*}

\begin{table*}[htbp]
  \begin{tabular}{ | c | c | c | }
    \hline
    \textbf{Event} & \textbf{Exact Duplicate Videos} & \textbf{Loose Duplicate Videos}\\
    \hline
    Assad Speech\footnote{3 videos were no longer available due to account termination by the users (two cases) and video takedown by the user (one case).}\todo{Footnote not shown. Needs manual fix?} & 0 videos in 0 sequences & 2 videos in 1 sequence\\
    Blackout SOPA & 2 videos in 1 sequence & 0 videos in 0 sequences\\
    CES Las Vegas & 0 videos in 0 sequences & 2 videos in 1 sequence\\
    Christian Wulff Case & 0 videos in 0 sequences & 0 videos in 0 sequences\\
    Costa Concordia & 0 videos in 0 sequences & 0 videos in 0 sequences\\
    Cut the Rope Launch & 0 videos in 0 sequences & 14 videos in 3 sequences\footnote{6 videos in 2 sequences were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Dixville Notch & 2 videos in 1 sequence & 0 videos in 0 sequences\footnote{1 video was no longer available due to video takedown by the user. 11 videos were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Free Mobile Launch & 0 videos in 0 sequences & 0 videos in 0 sequences\footnote{5 videos were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    Ubuntu TV Launch & 4 videos in 1 sequence & 9 videos in 4 sequences\footnote{8 videos in 3 sequences were false positives.}\todo{Footnote not shown. Needs manual fix?}\\
    \hline
  \end{tabular}
  \label{tab:duplicatevideos}
  \caption{Exact and loose duplicate videos per event.}
\end{table*}

\clearpage

\begin{figure*}
\begin{tabular}{p{\textwidth}}
\eventtitle{Blackout SOPA}
\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate4.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate6.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate7.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate8.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate9.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate10.png}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate11.jpg}
		\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate12.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\setlength\fboxsep{0pt}
		\setlength\fboxrule{0.1mm}
		\fbox{\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate13.png}}
		\fbox{\includegraphics[height=\thumbheight]{resources/sopa/looseduplicate14.jpg}}
\end{thumbsequence}
\end{tabular}

\vspace{.5em}
	
\begin{tabular}{p{\textwidth}}
\eventtitle{Christian Wulff Case}
	\begin{thumbsequence}
		\doublebox{\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate1.jpg}}
		\doublebox{\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate2.jpg}}
	\end{thumbsequence}
	\begin{thumbsequence}
		\doublebox{\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate3.jpg}}
		\doublebox{\includegraphics[height=\thumbheight]{resources/wulff/exactduplicate4.jpg}}
	\end{thumbsequence}
\end{tabular}

\vspace{.5em}

\begin{tabular}{p{.5\textwidth}p{.5\textwidth}}
\eventtitle{Dixville Notch}
	\begin{thumbsequence}
		\doublebox{\includegraphics[height=\thumbheight]{resources/dixville/exactduplicate1.jpg}}
		\doublebox{\includegraphics[height=\thumbheight]{resources/dixville/exactduplicate2.jpg}}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/dixville/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/dixville/looseduplicate2.jpg}
	\end{thumbsequence}
	&
\vspace{-3pt}
\eventtitle{Assad Speech}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/assad/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/assad/looseduplicate2.jpg}
	\end{thumbsequence}
\end{tabular}

\vspace{.5em}

\begin{tabular}{p{\textwidth}}
\eventtitle{Free Mobile Launch}
	\begin{thumbsequence}
		\doublebox{\includegraphics[height=\thumbheight]{resources/free/exactduplicate1.jpg}}
		\doublebox{\includegraphics[height=\thumbheight]{resources/free/exactduplicate2.jpg}}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate1.png}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate2.png}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate7.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate8.jpg}
	\end{thumbsequence}
	\\[4pt]
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate15.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate16.png}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate9.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate10.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate11.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate4.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate12.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate13.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate14.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/free/looseduplicate6.jpg}
	\end{thumbsequence}
\end{tabular}

\vspace{.5em}

\begin{tabular}{p{\textwidth}}
\eventtitle{Costa Concordia Disaster}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate2.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate4.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/concordia/looseduplicate6.jpg}
	\end{thumbsequence}
\end{tabular}

\vspace{.5em}

\begin{tabular}{p{\textwidth}}
\eventtitle{CES Las Vegas}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate4.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate5.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate6.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate7.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate8.jpg}
		\includegraphics[height=\thumbheight]{resources/ces/looseduplicate9.jpg}
	\end{thumbsequence}
\end{tabular}

\vspace{.5em}

\begin{tabular}{p{.5\textwidth}p{.5\textwidth}}
	\eventtitle{Cut the Rope Launch}
	\begin{thumbsequence}
		\doublebox{\includegraphics[height=\thumbheight]{resources/ropes/exactduplicate1.jpg}}
		\doublebox{\includegraphics[height=\thumbheight]{resources/ropes/exactduplicate2.jpg}}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate5.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate6.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate7.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate8.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate9.jpg}
	\end{thumbsequence}
	\newline\vspace{-.5em}\newline
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate12.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate13.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate14.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate15.jpg}
	\end{thumbsequence}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate10.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate11.jpg}
	\end{thumbsequence}
	\newstrip
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate2.jpg}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate3.png}
		\includegraphics[height=\thumbheight]{resources/ropes/looseduplicate4.png}
	\end{thumbsequence}
	&
\eventtitle{Ubuntu TV launch}
	\begin{thumbsequence}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate1.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate2.jpg}
		\newstrip
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate3.jpg}
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate4.jpg}
		\newstrip
		\includegraphics[height=\thumbheight]{resources/ubuntu/looseduplicate5.png}
	\end{thumbsequence}
\end{tabular}
\caption{\todo{caption}}
\label{fig:sequences}
\end{figure*}

\clearpage

\subsection{Results Interpretation and Discussion}
Looking at our dataset, it becomes evident that the amount of exact duplicate images and videos is inferior compared to the amount of loose duplicates.
On the one hand, this is owed to applied search operators that exclude reshared microposts like, e.g., ReTweets of the same tweet.
So in order for the same media item to appear twice in our results, two users have to author different microposts referencing the same media item\footnote{Using the same, or a different URL.} rather than just one user resharing a micropost from the other.
On the other hand, it is also owed to our strict definition of exact duplicate, which counts resized versions of the same media item as different.
We argue that this, while at the first sight counterintuitive,
makes sense as someone had to actively process the media item.
A good example is the Blackout SOPA event where the text in the first sequence in \autoref{fig:sequences} is identical, however,
the images have different file sizes, resolutions, and paddings.
This implies that someone has taken the initial image, processed it by, e.g.,
cropping it, and only then used the modified version.
A similar example is the second sequence in \autoref{fig:sequences} of the Costa Concordia Disaster, where the chimney got cut off.

In order to access extracted media items in the future,
it may not be necessary to just store their references.
Users can at any time terminate their accounts on social networks,
delete media items, or change their privacy settings.
In addition to that, social networks themselves can take media items down on government order or enforcing own policies.
We experienced no longer accessible media items in the context of the Assad Speech event.
To be on the safe side, only media items where explicitly the media item owner as well as the social network permit storage by third parties should be stored.
We did, at this point, not consider legal requirements, however,
do note that for potential future exploitation this is indispensable.

By clustering loose duplicate media items in sequences,
we aim at boosting diversity in the returned set of media items for a given event:
if there are many media items for an event,
in the longterm we try to only show the most relevant ones from each sequence.
This will require a media item ranking formula that will include a combination of different categories of ranking criteria:

\paragraph{Visual Features}
Extracted visual features from media items can help judge their quality.
Sharp is better than blurry, higher contrast is better than lower contrast, etc.
An interesting observation can be made with videos: videos with less or just one camera shot are more probable to be produced by amateurs (in tendency more authentic),
whereas videos with more shots are more probable to be produced by professionals (in tendency more official and credible).
Depending on the event, the one might be preferred over the other.

\paragraph{Low-level Features}
Common low-level features like resolution, file size, but also the presence of Exif data for, e.g.,
geolocation, are good quality indicators.
Longer videos are better than shorter videos, higher resolution is better than lower resolution, etc.

\paragraph{Social Features}
Media items belonging to the same sequence can have different popularity,
both globally across social networks, or on specific social networks.
Combining network-specific signals,
like number of ReTweets on Twitter, Likes on Facebook, +1s on Google+, views on YouTube;
and generic signals, like number of comments to a micropost,
a good social popularity indicator can be generated.
This can also include user diversity, i.e.,
featuring content from different users rather than just content from one user.

\paragraph{Textual Features}
Important context is available in form of microposts that accompany media items.
Named entity disambiguation can reveal valuable insights and, in combination with advanced techniques like face recognition,
help make the right media item choices.
If a user sends a micropost containing a media item about a concert, and if via named entity disambiguation the singer can be mapped back to a Linked Data concept,
the media item can be searched for potentially known objects, e.g., the singer's face.
Applying this combination of visual and textual features,
highly relevant media items can be selected.

\subsection{Result Representation}
Given that a media item ranking was available,
the question of result representation raises.
Should more popular media items be displayed bigger, longer, or with a special decoration like a thicker border in comparison to less popular media items?
For videos, the audio part poses a challenge.
In our experiments we have made the interesting finding that intermixing the audio of all videos of an event oftentimes generates a very characteristic ``noise cloud''.
A good example is the Assad Speech event, where a mix of Arabic voices blends nicely with the speech of a US politician.
A different example is the CES Las Vegas event, where the atmosphere of a big exposition with music, announcements, and technical analyses becomes alive.


% Responsible: Ruben
\section{Related Work} \label{sec:relatedwork}
% Separate related work in fields:
% - media item collection from social networks
% - event summarization

% Highlight what Twitter is doing (most popular image/video)
% Google news top stories
% Automatic gallery creation

A~first category of related work includes research that aims to collect, align, and organize media for trends or events.
Liu \emph{et al.} combine semantic inferencing and visual analysis to automatically find media to illustrate events~\cite{Liu2011}.
They interlink large datasets of event metadata and media with the Linking Open Data Cloud~\cite{LODcloud}.
Approaches for alignment use visual, temporal, and spacial similarity measures to map multiple photo streams of the same events~\cite{Yang2011}.
Other ways to collect and order media from social networks use user-driven metadata such as geospatial information~\cite{Crandall}.

Another relevant work area is duplicate and near-duplicate media detection.
As evident from several experiments and mentioned in \autoref{subsec:dataset},
providing unique media content to users is still an unsolved problem.
Work on ordinal measures for image correspondence started in the last decade of the 20\superscript{th}~century~\cite{Bhat}.
Recently, Chum \emph{et al.} have proposed a near-duplicate image detection method using MinHash and tf--idf weighting~\cite{Chum}.
A~method for both images and video has been proposed by Yang \emph{et al.}~\cite{Yang}.
Specialized methods for video exist as well~\cite{Min, Wu}, an excellent survey of which has been conducted by Lian \emph{et al.}~\cite{Lian}.

When unique media items have been collected, the remaining task is to summarize events by selecting the most relevant media fragments.
An article by Fabro and B\"osz\"orm\'enyi~\cite{Fabro2012} details the summarization and presentation of events from content retrieved from social media.
Nowadays, many domain-specific methods already exhibit good accuracy, for example in the sports domain~\cite{Li1,Li2}.
However, the challenge in this field is to find methods that are content-agnostic.
Methods that exploit semantic information~(\emph{e.g.}, \cite{Chen}) will likely provide high-quality results in the future,
but today's most relevant summaries are produced by user interaction~\cite{Olsen}.

% Temporarily added to simplify writing and reading Future Work section
%\clearpage

\section{Conclusion and Future Work}
% Conclusion
\todo{Write conclusion}

% Future work
% Responsible: Ruben
In this work, the focus has been on extracting visual media and associated textual messages from social networks.
One possibility for future work would therefore be to pursue our efforts in this direction by supporting more social networks and improving our Web scrapers.
This could significantly improve the quantity and diversity of considered media and messages,
given the fact that different kinds of information are shared to different networks~\cite{ConsumersLook}.

A~more innovative direction is to incorporate techniques from multimedia analysis into the process,
as this will create a~multi-modal environment where different factors are used to organize social content.
This can start with context-independent analyses, such as the content deduplication techniques discussed in \autoref{sec:relatedwork},
or visual quality metrics~(sharpness, contrast\ldots) to display original and high-quality media more prominent in results.
Furthermore, this identification of original content can allow users to choose a~balance between popularity~(favor omnipresent content) and originality~(promote rare content).

Context-aware multimedia analysis will likely bring a~new range of parameters into play,
since many media items contain a~message that is complementary to the text.
For example, facial detection~\cite{ViolaJones} and eventually recognition~\cite{Wright}
can signify the presence of specific people in a~media fragment.
As visual recognition systems grow more powerful, more objects will eventually be recognizable by machines~\cite{Serre},
which would allow generating \emph{visual hashtags} that describe the content \emph{inside} of the media item.
Extracted features in all three categories~(\emph{textual} -- from the micropost,
\emph{visual} -- from the media item,
and \emph{social} -- from the social network in the form of ReTweets, Likes, +1s\ldots)
can also
serve as ranking criteria, be it in isolation, or in combination by
introducing a~ranking formula.
As a~result, this would also positively influence the diversity of automated summarizations.

Nonetheless, it remains important to view the media and the associated text as a~whole,
since the text could convey a~sentiment about or an~explanation of the visual data.
Using named entity recognition~\cite{NERD,AddingMeaningToMicroposts}, the important semantic elements in the message text can be identified to build an understanding of its meaning.
The contents of the message could subsequently be used to narrow down the search space for visual factors, enabling cross-fertilization between the textual and visual analysis, which results in effective, context-aware analysis possibilities~\cite{verborgh_mtap_2011}.

\section*{Acknowledgments}
The research activities as described in this paper were funded by Ghent University, the Interdisciplinary Institute for Broadband Technology (IBBT), the Institute for the Promotion of Innovation by Science and Technology in Flanders (IWT), the Fund for Scientific Research Flanders (FWO Flanders), and the European Union.

This work was partially supported by the European Commission under Grant No. 248296 FP7 \mbox{I-SEARCH} project.
Joaquim Gabarr\'o is partially supported by TIN-2007-66523 (FORMALISM), and SGR 2009-2015 (\mbox{ALBCOM}).

% back to normal size Computer Modern for URLs in bibliography
\let\ttdefault\oldttdefault
\let\url\oldurl

\bibliographystyle{abbrv}
\bibliography{icmr2012}

\balancecolumns
\end{document} 