\documentclass[runningheads,a4paper]{llncs}

% proper encoding
\usepackage[T1]{fontenc}

\usepackage{flushend}

\usepackage{textcomp}
\usepackage{listings}
\lstset{frame=lines,captionpos=b,numberbychapter=false,escapechar=§,basicstyle=\ttfamily,upquote=true}

\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subfig}

% Thanks, http://lenaherrmann.net/2010/05/20/javascript-syntax-highlighting-in-the-latex-listings-package
\usepackage{color}
\definecolor{darkgray}{rgb}{.4,.4,.4}

\lstdefinelanguage{JavaScript}{
  keywords={push, typeof, new, true, false, catch, function, return, null, catch, switch, var, if, in, while, do, else, case, break},
  keywordstyle=\bfseries,
  ndkeywords={class, export, boolean, throw, implements, import, this},
  ndkeywordstyle=\color{darkgray}\bfseries,
  identifierstyle=\color{black},
  sensitive=false,
  comment=[l]{//},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{darkgray},
  stringstyle=\color{red},
  morestring=[b]',
  morestring=[b]"
}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage{hyperref}
\def\sectionautorefname{Section}

\begin{document}

\mainmatter

\title{On-the-fly Detection of Events in YouTube Videos}

% a short form should be given in case it is too long for the running head
\titlerunning{On-the-fly Detection of Events in YouTube Videos}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Thomas Steiner$^{1}$ \and Ruben Verborgh$^{2}$ \and Rik Van de Walle$^{2}$ \and Michael Hausenblas$^{3}$ \and Joaquim Gabarr\'{o} Vall\'{e}s$^{1}$}
\institute{Universitat Polit\'ecnica de Catalunya -- Department LSI\\
08034 Barcelona, Spain\\
\urldef{\emails}\path|{tsteiner,gabarro}@lsi.upc.edu|\emails
\and Ghent University -- IBBT, ELIS -- Multimedia Lab\\Gaston Crommenlaan 8 bus 201, B-9050 Ledeberg-Ghent, Belgium\\
\urldef{\emails}\path|{ruben.verborgh,rik.vandewalle}@ugent.be|\emails
\and DERI, NUI Galway IDA Business Park, Lower Dangan Galway, Ireland\\
\urldef{\emails}\path|michael.hausenblas@deri.org|\emails
}
%
\authorrunning{On-the-fly Detection of Events in YouTube Videos}
% (feature abused for this document to repeat the title also on left hand pages)

\maketitle
% \thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Even if considerable efforts have been put into making video content on the Web more accessible, searchable, and navigable by research on both textual and visual analysis of the actual video content and the accompanying metadata, most of the time videos still are relatively opaque objects in websites. With the ongoing process of Web browsers gaining more and more support for the HTML5 \texttt{video} tag, at least mark-up-wise videos are becoming first class citizens on the Web. In this paper, we show how through a combination of textual, visual, and behavioral analysis events can be detected on-the-fly in YouTube videos. We therefore define three types of events: \emph{visible events} in the sense of scene changes, \emph{occurrence events} in the sense of the appearance of a named entity, and \emph{interest-based events} in the sense of purposeful in-video navigation by users. We realize this by means of a Web browser extension that seamlessly integrates into the YouTube homepage. This extension highlights points of interest in the video, lists the contained named entities with their depictions, and splits a video in scenes. The main contribution of this paper is a generalizable framework for the automatic semantic annotation of HTML5 video content. Albeit the annotation task is quite processor-intensive, through our browser extension we demonstrate how the generation of such semantic annotations can be crowdsourced, so that while users lean back and watch videos, the machine does the heavy lifting meanwhile.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}
Official statistics~\cite{youtube:stats} from YouTube~\cite{youtube}, one of the biggest online video platforms owned by Google, state that more than 13 million hours of video were uploaded during 2010, and that 35 hours of video are uploaded every single minute. Given this huge amount of video content, it becomes evident that advanced search techniques are necessary in order to retrieve the few needles from the giant haystack. Closed captions allow for keyword-based in-video search, a feature announced in 2008~\cite{googlevideo}. Searching for a phrase like ``that's a tremendous gift'', a caption from Randy Pausch's famous last lecture titled \emph{Achieving Your Childhood Dreams}~\cite{pausch}, indeed reveals a link to that lecture on YouTube. If no closed captions are available, nor can be automatically generated~\cite{youtubecaptions}, keyword-based search is still available over tags, video descriptions, and titles. Presented with a potentially huge list of results, preview thumbnails based on video still frames help users decide on the most promising result.

A query for at the time of writing recent events like the London riots\footnote{\url{http://en.wikipedia.org/wiki/2011_London_riots}} or the shooting in Utøya\footnote{\url{http://en.wikipedia.org/wiki/2011_Norway_attacks}} reveals a broad selection of all sort of video content, both professionally produced and even more often shaky amateur videos taken with smartphones. Their thumbnails are typically very similar, as can be seen in~\autoref{fig:thumbnails}. These thumbnails are automatically generated by an unpublished computer vision-based algorithm~\cite{googleresearch}. From a user's point of view, it would be oftentimes very interesting to see whether a video contains different scenes. For example, a back-and-forth between a news anchorman and live images can be an indicator for professionally  produced content, whereas just one scene during the whole video can be an indicator for amateur-generated eyewitness footage.

\begin{figure}[htb!]
  \begin{center}
\subfloat[YouTube search for ``tariq jahan'', father of a victim of the London riots.]{\label{fig:thumbnails-a}\includegraphics[width=0.3\textwidth]{./resources/riots.png}}
\hspace{5pt}
\subfloat[YouTube search for ``utøya shooting'', scene of a terrible massacre in Norway.]{\label{fig:thumbnails-b}\includegraphics[width=0.3\textwidth]{./resources/utoya.png}}
  \caption{Similar thumbnails for different videos making the decision on a particular video harder. The shown user interface is from the iPhone YouTube application.}
  \label{fig:thumbnails}
  \end{center}  
\end{figure}

In addition to the information provided by the separation of a video in scenes, listing occurrences of named entities and their disambiguation can help users quickly decide whether a given video is of interest or not. For example, if a video about Utoya contains an occurrence of the Norwegian Prime Minister Jens Stoltenberg, or a video about the London riots contains an occurrence of the Prime Minister of the United Kingdom David Cameron, they can potentially be considered more trustworthy than other videos. While obviously the detection of persons and their identification would be possible through face detection and face recognition techniques, the task is very expensive. As we have shown in~\cite{semwebvid}, through the analysis of the available textual metadata with Natural Language Processing (NLP) techniques, however, good results are also possible, especially given the relatively broad availability of closed captions on YouTube, or their automatic generation~\cite{youtubecaptions} respectively.

Finally, for videos that are longer than the attention span of a typical YouTube user, exploiting purposeful in-video navigation data can help determine points of interest within videos. For example, many users might skip the intros typically contained in professionally produced video content, or jump to spectacular scenes directly. 

In this paper, we show a crowdsourced browser extension-based  approach for the on-the-fly event detection in YouTube videos through a combination of emph{textual}, \emph{visual}, and \emph{behavioral} analysis techniques. When a user starts watching a video, three processes start:
\begin{description}
\item[Visible Events] We detect scenes in the video by visually analyzing its content. We do this with the help of a browser extension, i.e., the whole process runs dynamically on the client-side, using the modern HTML5~\cite{w3c_html5} JavaScript APIs of the \texttt{<video>} and \texttt{<canvas>} elements. As soon as the scenes have been detected, we offer the user the choice to quickly jump into a specific scene by clicking on a representative still frame.
\item[Occurrence Events] We analyze the available video metadata using NLP techniques, as outlined in~\cite{semwebvid}. The detected named entities are presented to the user in list form, and upon click allow for jumping into the  scenes where the entity occurs.
\item[Interest-based Events] As soon as the \emph{visible events} have been detected, we attach JavaScript event listeners to each of the scenes and count clicks on scenes as an expression of interest in those scenes.
\end{description} 
 
\autoref{fig:youtube} shows the seamless integration of the detected events into the YouTube homepage. Contributions of this paper are the browser extension itself and the underlying generalizable framework for the automatic semantic HTML5 video annotation.

The remainder of this paper is structured as follows: lorem ipsum.

\begin{figure*}
\begin{center}
   \includegraphics[width=0.8\linewidth]{./resources/youtube.png}
\end{center}
   \caption{Screenshot of the YouTube browser extension, showing the three different event types: \emph{visible events} (video scenes below the video), \emph{occurrence events} (contained named entities and their depiction at the right of the video), and \emph{interest-based events} (points of interest in the video highlighted with a red background in the bottom left).}
\label{fig:youtube}
\end{figure*}

\section{Related Work} \label{sec:related-work}
% Ruben
Lorem ipsum.

\section{Implementation Details} \label{sec:implementation}
We begin this Section with an overview of the technologies used, as we introduce Google Chrome extensions for the Google Chrome browser and give a brief summary of the Web analysis solution Google Analytics.

\subsection{Google Chrome Extensions}
Google Chrome extensions~\cite{chromeextensions} are small software programs that users can install to enrich the browsing experience with the Google Chrome~\cite{chrome} browser. They are written using a combination of standard Web technologies, such as HTML, JavaScript, and CSS. Chrome extensions are usually (but not necessarily) distributed through the Chrome Web Store~\cite{chromewebstore}. There are several types of extensions; for this paper we focus on extensions based on so-called content scripts. Content scripts are JavaScript programs that run in the context of Web pages via dynamic code injection. By using the standard Document Object Model (DOM), they can read or modify details of the Web pages a user visits.

\subsection{Google Analytics}
Google Analytics~\cite{googleanalytics} is Google's Web analysis solution allowing for detailed statistics about the visitors of a website. The software is implemented by adding an invisible snippet of JavaScript code on the to-be-tracked pages of a website. This code collects visitor data through a request for a specific $1 \times 1$ pixel image, during which the page and user data is reported back in the query part of the image's URL. In addition to that, the snippet sets a first party cookie on visitors' computers in order to store anonymous information such as the timestamp of the current visit, whether the visitor is a new or returning visitor, and the website the visitor came from.

\section{Future Work and Conclusion} \label{sec:future-work-conclusion}
Lorem ipsum.

% back to normal size Computer Modern for URLs in bibliography
\renewcommand{\ttdefault}{cmvtt}
\renewcommand\UrlFont\tt

\bibliographystyle{splncs03}
\bibliography{derive2011}

\end{document}